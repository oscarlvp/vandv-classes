== Software Testing

According to Ammann and Offutt <<amman2017introduction>> _Software Testing_ is the process of evaluating a software by observing its execution to reveal the presence of faults or as Meyer says in his principles <<meyer2008seven>> _"`to test a program is to try to make it fail`"_. Per this definition, testing is a form of dynamic analysis, as it requires the execution of the program or system under test. The testing process is achieved with the design and application of test cases. 

In broad terms, a test case must provide the required _input_ and set the program in the desired state by _triggering specific behaviors_. It then, must _check the output_ of the program against expected results. Often, the output of the test execution is validated with an _oracle_, a predicate to tell whether the execution was successful or not. A test case may optionally include _prefix values_ which are inputs necessary to get the system ready for the input and _postfix values_ which are inputs needed to clean the environment after the execution of the test. Test cases may be _functional_, if it checks that a functionality is correct, or _non-functional_ if it is directed to evaluate properties like performance, security or even energy consumption. If a test case executes normally it is said to _pass_ otherwise, if the output is not correct we say that there is a _test failure_ and that the test _fails_.

A test case exercises the system in one specific scenario. So, only one test case is not enough to correctly verify the system. Therefore we always create a set of multiple test cases usually called _test suite_. 

=== Levels of testing

Testing can (and should) be done at different levels of abstractions <<amman2017introduction>> <<aniche-software>>:

Unit Testing:: It is the lowest level of testing and targets the code units of the program: procedures, functions or methods. The goal is to assess the _implementation_ of the software. For example, a unit test for a function that computes the factorial of a given number, would call the function with an input, say 5 and then check that the result is 120. 

Module Testing:: Units are often grouped in _modules_ that could be classes, packages or files. This level of testing verifies each module in isolation and evaluates how the component units interact with each other. For example, a test for a `Stack` class may create an instance of the class, push an element onto the stack, pop it out and finally check that the stack is empty.

NOTE: In Object-Oriented Programming a class is considered as both a unit and a module. Therefore the distinction between _unit tests_ and _module tests_ is not clear within this context. In fact, for most testers and developers both types of testing are known as _unit testing_ and it will be also the case for the rest of this text. 

Integration Testing:: The goal of this level of testing is to assess the interaction between modules and verify that they are communicating correctly. Integration tests assume that modules work correctly, that is the task of the previous testing levels. As an example, an integration test for an online store may check the interaction between the shopping cart module and the payment gateway by creating and validating an order, or it could also verify that data access abstractions communicate correctly with the database by creating the right queries and interpreting the results in the right way.

System Testing::  The goal of this level is to check whether the assembled system meets the specifications thus, It assumes that all subsystems work correctly, which is the task of the previous testing levels. Integration tests execute the system as a whole from beginning to end, that is why they are sometimes also referred as _end-to-end_ tests. An example of such tests in the context of an online store could verify that: a user can log in with the right credentials, she can create a shopping cart with a selection of products, she can validate the order, she receives an email with the invoice and then she is able to log out. Testing done from the graphical user interface is also a for of system testing.

Acceptance Testing:: This level checks that the finished product meets the needs of the users, that is, if the system does what they want. These tests should be elaborated from the user's perspective and may be used to evaluate the requirements. Like system tests, these also verify the product from beginning to end.

NOTE: There might be other, even finer-grained, classifications for tests and they all tend to be blurred sometimes ambiguous. Classifying one test case as unit, module, integration or system is not crucial. The most important message here is that testing should be carried at different levels, for each unit, whatever they may be, then how these units interact and then how the entire system behaves.

Each level of testing has its own level of complexity. Unit tests tend to be easier to define and faster to execute. System tests are more complex and slower. Inputs for unit tests may not be completely realistic compared to the inputs that shall appear in production and they are not generally able to find faults arising from the interaction of modules. System tests tend to be more realistic  Also finding an implementation fault from a system test may be even impractical.

Automating the execution of tests at all levels is key for modern software development practices. Manual tests are tedious, take a lot of time and are prone to errors. Automated test execution allows having more tests and therefore more scenarios are explored. As Meyer explains in his fourth principle,<<meyer2008seven>> it is not feasible to manually check the output of hundreds of tests. Automated tests help producing a faster, almost immediate feedback during development on whether certain types of faults are being inserted. In this way they enable efficient CI/CD processes which facilitate the integration of the work of development teams. Automated tests help also preventing _regressions_ that is, the reinsertion of an already solved fault. This is specially true if we adhere to Meyer's third principle "`Any failed execution must yield a test case, to remain a permanent part of the project’s test suite`" <<meyer2008seven>>.

The accepted practice nowadays is to have most tests automated, while keeping only a small amount of manual tests, mainly as acceptance tests. Most organizations have a large number of unit tests, which are easier to write, faster to execute and easier to understand and less system tests which are more complex, expensive and harder to maintain. This is reflected on what is known as the _testing pyramid_ <<testing-pyramid>>.

[#testing-pyramid.text-center]
.The testing pyramid.
image::testing-pyramid.png[Testing pyramid, 600]

So, as we move in the pyramid from unit tests, which directly assess the implementation of the system,  up to acceptance tests which check the compliance of the system with the users' needs, we mode from verification to validation.

=== Test automation frameworks: JUnit

Test automation allows having more tests to explore more execution scenarios, provides a faster feedback and facilitates integration processes. It is achieved with the help of _test automation frameworks_ or _testing frameworks_.

A testing framework is a set of tools and libraries providing mechanisms to define or specify test cases and execute them. One of the most famous alternatives at the moment is _JUnit_, initially developed by Kent Beck back in 1997. JUnit has become a sort of _de-facto_ standard for Java projects and has inspired the creation of similar frameworks for other languages and platforms which are informally called as the _xUnit_ family.
Despite hading "`unit`" in the name and being widely used for unit testing, the framework can be use to implement all sort of automated tests.

NOTE: At the moment of writing this material the latest stable version of JUnit is 5.6.2. This version will be used for all code examples.

Suppose we have a `Stack` class that implements a _LIFO_ (Last In First Out) data structure. The class has a `void` method `push` to insert an element onto the stack and `pop` that removes the element on top of the stack and returns it. A typical unit test for this class written with the help of JUnit would look like the code shown in <<junit-example>>.

[[junit-example, Listing {counter:listing}]]
.Listing {listing}. A typical unit test written with JUnit.
[source,java]
----
class StackTest {
    @Test
    public void testPushPop() {
        int original = 1;
        Stack stack = new Stack();
        stack.push(original);
        int onTop = stack.pop();
        assertSame(item, onTop, "Element on top of the stack should be " + original);
    }
}
----

Test cases in JUnit are implemented inside _test classes_. These classes declare _test methods_ which contain the main code for the test cases. These test methods are identified with the `@Test` annotation. In <<junit-example>> the first four lines of `testPushPop` provide the input values of the test case and set the instance of `Stack` in the required state: an element has been pushed and then popped from the stack. 

The last line uses an oracle to verify that the element obtained from the stack was the same that was pushed in the first place. This type of oracle is known as an _assertion_. It evaluates a given condition and if the condition is false an `AssertionError` is thrown. It also includes a message to use as output in the case the assertion fails. In the absence of any assertion in the code, JUnit tests have an implicit oracle that checks if unexpected errors occur, that is, if an unexpected exception is thrown.

JUnit provides a set of utility methods implementing different assertions such as: `assetEquals` that checks if two given objects are equal, `assertNotEqual`, the contrary, `assertNull` which verifies if a given value is `null` or not, `assertSame` used in the example to verify if two objects are the same and many more.

In some scenarios, a test case should verify whether an operation with the wrong input signals the right error. <<junit-throw>> shows how to achieve this. The test verifies that invoking `pop` in an empty `Stack` should throw an `IllegalOperationException`.

[[junit-throw, Listing {counter:listing}]]
.Listing {listing}. Verifying the correct error with JUnit.
[source,java]
----
@Test
public void testErrorPopEmptyStack() {
    assertThrows(IllegalOperationException.class, () -> {
        new Stack().pop();
    });
}
----

While the assertions included in JUnit cover a wide spectrum of scenarios, libraries like http://hamcrest.org/JavaHamcrest/[Hamcrest] and https://joel-costigliola.github.io/assertj/[AssertJ] help creating more expressive and higher level assertions.

A test case in JUnit could be more than a single test method, it may include other methods supporting the test execution. For example, methods annotated with `@BeforeEach` and `@AfterEach` will be executed before and after each identified test cases in the same test class respectively. These are helpful to set prefix and postfix test inputs.

JUnit includes many additional functionalities to facilitate the creation of tests, such as parameterized tests, special oracles to verify the performance of the code and even the dynamic creation of tests.

It is important to add information that helps identifying the fault in the event of a test failure. In JUnit, and any other testing framework, a common practice to try to achieve this is to use descriptive names for test methods and set detailed messages for assertions. However, there are many other characteristics that good test cases must have in practice.

=== Best practices and antipatterns in testing

Automated test cases are code, _test code_, and like the _application code_ they should be maintained and we should care about their quality. Poorly written test cases bring no value to the development process. They negatively impact the fault detection capabilities of a test suite. They are also hard to understand and hard to leverage when identifying faults.

As summarized in <<meszaros2003test>> automated test cases should be *concise* and *clear*: brief yet comprehensive and easy to understand, *self-checking*: they should report results without human intervention, *repeatable*, *robust* and *independent*: it should be possible to run them consecutive times without human intervention and they should always produce the same results whether their are run in isolation or with other tests. Tests should also be *efficient*: they should be run in a reasonable amount of time and should be *maintainable*: that is, they must be easy to modify and extend even when the system under test changes. Also, with respect to the application and the requirements, tests must be *sufficient* so all requirements of the system are verified, *necessary* so that everything in each test contributes to the specification of the desired behavior, with no redundancy and unneeded artifacts, each test should be *specific* so tests failures point to the specific fault, and *traceable* so that it can be easily mapped to the parts of the application code it verifies and the part of the specification it has been derived from. 

==== Test smells

Along the years, the testing community has identified bad practices, _smells_ that deviate from the principles mentioned above and impact the quality of tests. Garousi and Küçük <<garousi2018smells>> reviewed the scientific and industry literature on the subject and were able to identify 179 different test smells. It is important to notice that test smells are not bugs but affect the tests by lowering their efficiency, maintainability, readability, comprehension and their ability to find faults. This section presents and exemplifies some of these test smells.

Manual intervention:: Happens when the person running the test case must do something manually before the test is run, during the execution or should manually verify the results. This goes in detriment of test automation.

Testing Happy Path only:: Tests only verify the common scenario, and never check boundaries or input values that should result in exceptions. Most of the time developers program with the happy path / normal situation in mind and it is most likely that this scenario will work. Therefore testing the happy path only have lower chances to catch a bug. The test case in <<junit-example>> tests only the most expected scenario or happy path. We need to add test cases like <<junit-throw>> where we explore extreme scenarios like a pop on an empty stack or when a null element is pushed or if if there is a point at which we can push no more elements to the stack.

Test logic in production code:: The application code put into production contains logic that should be exercised only during test execution. This logic has been put there only to support testing, for example, to help tests gain access to the internal state of the application. It also may happen that part of the production logic can not be executed in testing. This makes the system behaves differently in production and testing. An example is shown in <<test-logic-in-production>>.

[[test-logic-in-production, Listing {counter:listing}]]
.Listing {listing}. Example of test logic in production code.
[source,java]
----
...
if (System.getProperty("env", "prod").equals("test")) {
    return new User('Jane Doe', 'janedoe@example.com'); //<1>
}
else {
    User user = new User(Request.getParam(login), Request.getParam(name));
    validateUser(user);
    return user;
}
...
----
. <1> This line makes the code return a wired values to use in production.

Another example of this type of smell is when a class does not require an implementation of `equals` and we do need it just for testing purposes. This is known as _equality pollution_. The application code is filled with unnecessary `equals` methods, whose logic may actually go against the requirements. 

In general, all forms of this test smell make the application code more complex and introduces maintainability issues.

A way to solve this smell, is to use _dependency injection_. The code that has to work differently in production and tests can be moved onto a dependency that can be exchanged without affecting the application logic. In case of equality pollution we could use an _equality comparer_, that is, a class that checks if two objects are equals per our needs.

Eager test:: Also known as *The Test It All* or *Split Personality*. It is a single test that verifies too many functionalities. <<eager-roulette>> shows an example of this test smell. When such a test fails, it is hard to tell which is the actual functionality that contains the fault. The solution is to separate all verifications into different test cases.

[[eager-roulette, Listing {counter:listing}]]
.Listing {listing}. An example of a test that tries to test too much in the same test case (Eager Test) and it is also hard to know the fault in the presence of a test failure. Taken from <<xunitpatterns-assertion>>.
[source,java]
----
@Test
public void testFlightMileage_asKm2() throws Exception {
    // setup fixture
    // exercise contructor
    Flight newFlight = new Flight(validFlightNumber);
    // verify constructed object
    assertEquals(validFlightNumber, newFlight.number);
    assertEquals("", newFlight.airlineCode);
    assertNull(newFlight.airline);
    // setup mileage
    newFlight.setMileage(1122);
    // exercise mileage translater
    int actualKilometres = newFlight.getMileageAsKm();    
    // verify results
    int expectedKilometres = 1810;
    assertEquals( expectedKilometres, actualKilometres);
    // now try it with a canceled flight:
    newFlight.cancel();
    try {
        newFlight.getMileageAsKm();
        fail("Expected exception");
    } catch (InvalidRequestException e) {
        assertEquals( "Cannot get cancelled flight mileage", e.getMessage());
    }
}
----

Assertion roulette:: Appears when it is hard to tell which of the many assertions of  a test method produced the test failure. This makes harder to diagnose the actual fault. Eager tests tend to also produce assertion roulettes as can be seen in <<eager-roulette>>. This smell also occurs when assertions do not have any message, as seen in <<no-message-assertion>>. To solve this smell we should refactor the test code and add a descriptive message to all assertions.

[[no-message-assertion, Listing {counter:listing}]]
.Listing {listing}. Example of a test case with several assertions with no message. In the case of a test failure it is hard to know which assertion failed and to diagnose the fault. Taken from <<xunitpatterns-assertion>>
[source,java]
----
@Test
public void testInvoice_addLineItem7() {
    LineItem expItem = new LineItem(inv, product, QUANTITY);
    // Exercise
    inv.addItemQuantity(product, QUANTITY);
    // Verify
    List lineItems = inv.getLineItems();
    LineItem actual = (LineItem)lineItems.get(0);
    assertEquals(expItem.getInv(), actual.getInv());
    assertEquals(expItem.getProd(), actual.getProd());
    assertEquals(expItem.getQuantity(), actual.getQuantity());
}
----

The Free Ride:: Also known as *Piggyback* and closely related to the two previous test smells. In this smell, rather than write a new test case method to test another feature or functionality, testers add new assertions to verify other functionalities. It can lead to eager tests and assertion roulettes. As with this two other smells, piggybacking makes it hard to diagnose the fault. <<piggybacking>> shows an actual example of this smell from the Apache Commons Lang project.

[[piggybacking, Listing {counter:listing}]]
.Listing {listing}. Actual example of the piggybacking test smell. Code can be consulted in the https://github.com/apache/commons-lang/blob/649dedbbe8b6ab61fb3c4792c86b7e0af7ec4a73/src/test/java/org/apache/commons/lang3/ArrayUtilsRemoveMultipleTest.java#L34[Apache Commons Lang code repository]. This is also an example of an eager test and assertion roulette.
[source,java]
----
@Test
public void testRemoveAllBooleanArray() {
    boolean[] array;

    array = ArrayUtils.removeAll(new boolean[] { true }, 0);
    assertArrayEquals(ArrayUtils.EMPTY_BOOLEAN_ARRAY, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 0);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 1);
    assertArrayEquals(new boolean[]{true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true }, 1);
    assertArrayEquals(new boolean[]{true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 0, 1);
    assertArrayEquals(ArrayUtils.EMPTY_BOOLEAN_ARRAY, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 0, 1);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 0, 2);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 1, 2);
    assertArrayEquals(new boolean[]{true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 0, 2, 4);
    assertArrayEquals(new boolean[]{false, false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 1, 3);
    assertArrayEquals(new boolean[]{true, true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 1, 3, 4);
    assertArrayEquals(new boolean[]{true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 0, 2, 4, 6);
    assertArrayEquals(new boolean[]{false, false, false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 1, 3, 5);
    assertArrayEquals(new boolean[]{true, true, true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 0, 1, 2);
    assertArrayEquals(new boolean[]{false, true, false, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());
}
----

Interacting Tests:: Tests that depend on each other in some way. It may happen when one test depends of the outcome of another, for example, as a result of a test, a file is created which is used to execute another test. In this way a test may fail for reasons other than a fault in the behavior it is verifying.

The Local Hero:: A test case depends on something specific to the development environment. It passes in a matching environment but fails under any other conditions. This may happen when tests depend on the existence of specific services or even machine features. Such assumptions should always be avoided.

Conditional test logic:: Also known as *Guarded Test*. Consists in a test that contains code that may or may not be executed. It makes tests more complicated than actually needed and therefore less readable and maintainable. It usually appears with the use of control structures within a test method. <<conditional-logic>> shows an example.

[[conditional-logic,Listing {counter:listing}]]
.Listing {listing}. An example of conditional logic in a test. In this case, if the element is not returned by the iterator, the test executes without failing.
[source, java]
----
 //  verify Vancouver is in the list:
    actual = null;
    i = flightsFromCalgary.iterator();
    while (i.hasNext()) {
        FlightDto flightDto = (FlightDto) i.next();
        if (flightDto.getFlightNumber().equals( expectedCalgaryToVan.getFlightNumber())) //<1>
        {
            actual = flightDto;
            assertEquals("Flight from Calgary to Vancouver", expectedCalgaryToVan, flightDto);
            break;
        }
    }
}
----
<1> Checks the presence of an element. If the element is not there, then the test executes and does not fail.

Fragile test:: A test that fails to compile or run when the system under test is changed in ways that do not affect the part the test is exercising. These tests increase the cost of maintenance. There are many causes for this smell, so code should be carefully inspected and refactored.

Erratic tests:: Also known as *Flaky Tests*. Test that behave erratically, sometimes they fail and sometimes they don't under the same conditions. These tests undermine the trust developers have on their test suites. It is hard to know whether the failure is due to an actual fault or not. There are many reasons this could happen, for example, the already mentioned *Interacting Tests* smell, incorrect handling of the resources the test should use, and any type of non-determinism in tests coming from race conditions, synchronization, concurrency, time-outs and randomly generated data. Erratic or flaky tests are more common in higher level testing such as integration or system tests. They are a true plague for companies that develop big systems. As an example, Google has reported that nearly 1.5% of their tests behave erratically  <<micco2016flaky>>. 

Get really clever and use random numbers in your tests:: Using randomly generated data in tests cases is not necessarily a bad idea. Random tests can discover cases that developers have missed. However, random data has to be carefully managed to avoid creating *Erratic Tests* and to ensure that tests can be *repeatable*. Some of the key actions to consider are, to use pseudo-random numbers and store or log the seed used to generate the data and log the data used in case of a failure, so the test can be repeated in a posterior moment to diagnose the fault.

Testing private methods:: Also known as  *X-Ray Specs*. Tests should verify results, not the implementation. Results are most likely to remain the same even when the implementation changes, also results come from the specification the implementation should follow. Private  methods are implementation artifacts hidden from external users. Verifying results should only involve the public API and not knowing the internals of a module or class. Also, trying to test private methods, requires a non-trivial plumbing, that would make tests more complicated. So, tests should not directly target private methods, but they _must assess their effect_ through the public API.

==== Real examples of good testing practices

The previous sections presented the features a good test should have and described a selection of common antipatterns in testing. This section presents examples of good testing practices in Apache Commons Math, a serious open-source Java library quite used and popular. 

For each: presence of good features, and kinds of antipatterns they avoid


<<testing-exceptional-case>> shows a good example on how to handle the verification of exceptional cases and avoiding to test only the happy path. In this example, developers did two things. First the use of an special assertion for when the exception should expected, which can be seen in the annotation. This example uses JUnit 4. In JUnit 5, `assertThrows` is preferred. Also, they used the special assertion `fail` to mark the part of the test code that should not be executed if the exception is not thrown. In this way they ensure that the test will not silently pass in the event of any faut. The original code can be seen https://github.com/apache/commons-math/blob/eb57d6d457002a0bb5336d789a3381a24599affe/src/test/java/org/apache/commons/math4/filter/KalmanFilterTest.java#L43
[here].

[[testing-exceptional-case, Listing {counter:listing}]]
.Listing {listing}. A test case, testing the exceptional case, notice the use of `fail` to avoid finishing the test silently.
[source, java]
----
// In org.apache.commons.math3.filter.KalmanFilterTest
@Test(expected=MatrixDimensionMismatchException.class) //<1>
public void testTransitionMeasurementMatrixMismatch() {
    // A and H matrix do not match in dimensions
    // A = [ 1 ]
    RealMatrix A = new Array2DRowRealMatrix(new double[] { 1d });
    // no control input
    RealMatrix B = null;
    // H = [ 1 1 ]
    RealMatrix H = new Array2DRowRealMatrix(new double[] { 1d, 1d });
    // Q = [ 0 ]
    RealMatrix Q = new Array2DRowRealMatrix(new double[] { 0 });
    // R = [ 0 ]
    RealMatrix R = new Array2DRowRealMatrix(new double[] { 0 });

    ProcessModel pm
        = new DefaultProcessModel(A, B, Q,
                                    new ArrayRealVector(new double[] { 0 }), null);
    MeasurementModel mm = new DefaultMeasurementModel(H, R);
    new KalmanFilter(pm, mm);
    Assert.fail("transition and measurement matrix should not be compatible"); //<2>
}
----
<1> Annotation with an assertion to indicate that a `MatrixDimensionMismatchException` should be thrown.
<2> This line must not be executed, if the exception is properly thrown. This is therefore a safeguard ensuring that the test should fail in case this line is executed.


<<random-data>> shows and example on the use of random data in testing. Developers fixed the seed to generate the random numbers. It could be argued that this is in fact not random data, as the same numbers will be generated every time. However, this test reflects that the actual numbers play no role in the behavior being tested. On the other hand, the code is an example of a test case that should be divided in two. The actual code can be checked https://github.com/apache/commons-math/blob/eb57d6d457002a0bb5336d789a3381a24599affe/src/test/java/org/apache/commons/math4/linear/BlockFieldMatrixTest.java#L182[here].


[[random-data, Listing {counter:listing}]]
.Listing {listing}. Right use of random data. The test case fixes the seed, however it could be argued that it is in fact not exactly random.
[source, java]
----
// In org.apache.commons.math3.linear.BlockFieldMatrixTest

/** test copy functions */
@Test
public void testCopyFunctions() {
  Random r = new Random(66636328996002l); //<1>
  BlockFieldMatrix<Fraction> m1 = createRandomMatrix(r, 47, 83);
  BlockFieldMatrix<Fraction> m2 = 
	new BlockFieldMatrix<Fraction>(m1.getData());
  Assert.assertEquals(m1, m2);
  BlockFieldMatrix<Fraction> m3 = 	
	new BlockFieldMatrix<Fraction>(testData);
  BlockFieldMatrix<Fraction> m4 = 
	new BlockFieldMatrix<Fraction>(m3.getData());
  Assert.assertEquals(m3, m4);
}
----
<1> Using a fixed seed to ensure repeatability.

<<distribution-test>> provides an additional example of the use of random data. In this case developers are actually testing a random generator which should build a collection of vectors uniformly distributed around the unit sphere. Again, developers used a fixed seed. This test case also exemplifies the use of a good strong oracle, that validates the property of the distribution without assumptions on the actual numbers in use. Changing the seed it is most likely that will not change the result or make the test fail unnecessarily. The original code can be checked https://github.com/venkateshamurthy/java-quantiles/blob/master/src/test/java/org/apache/commons/math3/random/UnitSphereRandomVectorGeneratorTest.java#L29[here].

[[distribution-test, Listing {counter:listing}]]
.Listing {listing}. Another example on the use of random data. This time, he test case also has a strong verii
[source, java]
----
// In org.apache.commons.math3.random.UnitSphereRandomVectorGeneratorTest
@Test
public void test2DDistribution() {
    
    RandomGenerator rg = new JDKRandomGenerator();
    rg.setSeed(17399225432l); //<1>
    UnitSphereRandomVectorGenerator generator = new UnitSphereRandomVectorGenerator(2, rg);

    // In 2D, angles with a given vector should be uniformly distributed
    int[] angleBuckets = new int[100];
    int steps = 1000000;
    for (int i = 0; i < steps; ++i) {
        final double[] v = generator.nextVector();
        Assert.assertEquals(2, v.length);
        Assert.assertEquals(1, length(v), 1e-10);
        // Compute angle formed with vector (1,0)
        // Cosine of angle is their dot product, because both are unit length
        // Dot product here is just the first element of the vector by construction
        final double angle = FastMath.acos(v[0]);
        final int bucket = (int) (angleBuckets.length * (angle / FastMath.PI));
        ++angleBuckets[bucket];
    }
    // Simplistic test for roughly even distribution
    final int expectedBucketSize = steps / angleBuckets.length;
    for (int bucket : angleBuckets) { //<2>
        Assert.assertTrue("Bucket count " + bucket + " vs expected " + expectedBucketSize,
                            FastMath.abs(expectedBucketSize - bucket) < 350);
    }
}
----
<1> Fixed seed
<2> Strong verification


<<strong-data>> is an example of a test case with extensive data that has been carefully crafted to meet the requirements. The input data has been generated beforehand, possibly to ensure efficiency and repeatability. The generation process has been also carefully documented. The full test case can be seen https://github.com/venkateshamurthy/java-quantiles/blob/1dd682e8a00af5968ec4057b0613dd73d5eb704f/src/test/java/org/apache/commons/math3/special/GammaTest.java#L170[here].

[[strong-data, Listing {counter:listing}]]
.Listing {listing}. Example of carefully crafted input.
[source, java]
----
//In org.apache.commons.math3.special.GammaTest

    /**
     * Reference data for the {@link Gamma#logGamma(double)} function. This data
     * was generated with the following <a
     * href="http://maxima.sourceforge.net/">Maxima</a> script.
     * <pre>
     * kill(all);
     * fpprec : 64;
     * gamln(x) := log(gamma(x));
     * x : append(makelist(bfloat(i / 8), i, 1, 80),
     *     [0.8b0, 1b2, 1b3, 1b4, 1b5, 1b6, 1b7, 1b8, 1b9, 1b10]);
     * for i : 1 while i <= length(x) do
     *     print("{", float(x[i]), ",", float(gamln(x[i])), "},");
     * </pre>
     */
    private static final double[][] LOG_GAMMA_REF = {
        { 0.125 , 2.019418357553796 },
        { 0.25 , 1.288022524698077 },
        { 0.375 , 8630739822706475 },  
        //...129 more lines
    };
----

<<custom-assertion>> shows an example of a custom assertion, built to support the testing process. This is a verification used in several test cases inside the test suite. So, it is a good practice to refactor the assertion condition into a method. This is also a way to avoid *Equality Pollution*. In this case, even the JUnit style have been respected. Also notice how `doubles` are compared using a precision. Floating point types should never be compared with direct equality due to numerical errors. The code can be checked https://github.com/joulupunikki/math/blob/master/src/test/java/org/apache/commons/math3/TestUtils.java#L165[here].

[[custom-assertion, Listing {counter:listing}]]
.Listing {listing}. A custom assertion.
[source, java]
----
// In org.apache.commons.math3.TestUtils

    /**
     * Verifies that the relative error in actual vs. expected is less than or
     * equal to relativeError.  If expected is infinite or NaN, actual must be
     * the same (NaN or infinity of the same sign).
     *
     * @param msg  message to return with failure
     * @param expected expected value
     * @param actual  observed value
     * @param relativeError  maximum allowable relative error
     */
    public static void assertRelativelyEquals(String msg, double expected,
            double actual, double relativeError) {
        if (Double.isNaN(expected)) {
            Assert.assertTrue(msg, Double.isNaN(actual));
        } else if (Double.isNaN(actual)) {
            Assert.assertTrue(msg, Double.isNaN(expected));
        } else if (Double.isInfinite(actual) || Double.isInfinite(expected)) {
            Assert.assertEquals(expected, actual, relativeError);
        } else if (expected == 0.0) {
            Assert.assertEquals(msg, actual, expected, relativeError);
        } else {
            double absError = FastMath.abs(expected) * relativeError;
            Assert.assertEquals(msg, expected, actual, absError);
        }
    }

----

=== Test design

Any testing process, automatic or manual, could be abstracted as <<testing-process>>  shows. The system or program under test is executed using selected test inputs. The result of the execution is evaluated with the help of an oracle based on the specification. If the oracle deems the result incorrect, then we must find the fault. Otherwise, we continue the testing process until a stopping criterion is met. 

[graphviz, testing-process, png]
.Testing process at a glance. 
....
digraph {

    input[shape="rectangle", label="Test input"];
    program[shape="rectangle", label="Program under test"];
    execution[shape="polygon", sides="6", label="Execution"];
    result[shape="rectangle", label="Result"];
    specification[shape="rectangle", label="Specification"];
    oracle[shape="polygon", sides="6", label="Oracle"];
    verdict[shape="diamond", label="Verdict"];
    stop[shape="rectangle", label="Stopping Criteria"];
    fix[shape="plaintext", label="Locate and fix the fault"];

    input -> execution;
    program -> execution;
    execution -> result;
    result -> oracle;
    specification -> oracle;
    oracle -> verdict;
    { rankdir=LR; verdict -> fix; }
    verdict -> stop;
    stop -> input[label="Not met"];
}
....

This diagram puts in evidence three main problems to be solved when designing our tests. First we need to identify a set of test inputs that will become the starting point for the tests cases. Then, for each test case we need to define a strong oracle able to tell when the result of the execution meets the requirements or not. Also we need to know how much should we test, that is, to set a stopping criterion. Ideally we would test until there are no more faults or when we explore all possible inputs, but this can not be done in practice. Locating and fixing an identified fault is also a very important problem, but it is out of the scope of testing. However, tests failures should provide enough information for developers to find and correct the fault.

Solving these problems is far from easy. Whatever decision is made, it should have as ultimate goal to design tests capable of discovering faults.

==== Reachability, Infection, Propagation, Revealability

The main goal of testing is to reveal the presence of faults. However, there are concrete conditions test cases should meet in order to be able to observe a fault. These are expressed in the _Reachability, Infection, Propagation, Revealability_ (RIPR) model  <<li2016test>>.

<<ripr-faulty-program>> shows the `findLast` method. It should return the index of the last occurrence of a given element in a given array. If the element is not present the method should return -1 and if the array is `null` it should throw a `NullPointerException` exception. The method in question has a fault, the loop condition should be `i >= 0` otherwise the element in the first position is never inspected. Test cases in <<ripr-no-reachability>>, <<ripr-no-infection>>, <<ripr-no-propagation>>, <<ripr-no-revealability>> fail to observe the bug for different reasons.

[[ripr-faulty-program, Listing {counter:listing}]]
.Listing {listing}. `findLast` is supposed to return the index of the last occurrence of a given element in a given array. If the array is `null` the method should throw a `NullPointerException` exception. If the element is not present, then it should return -1. This method contains a fault as it never inspects the first element of the array.
[source, java]
----
public static int findLast(int[] array, int element) {
    for (int i = array.length - 1; i > 0 ; i--) { //<1>
        if(array[i] == element)
            return i;

    }
    return -1;
}
----
<1> Loop condition should be `i >= 0`. 

Reachability::

For a test case to discover a fault it must first execute/reach, the code location where the bug is present. The test case in <<ripr-no-reachability>> tests the behavior of the method when the input array is `null`. Therefore this test case never executes the loop condition and does not reach the fault.  
+
[[ripr-no-reachability, Listing {counter:listing}]]
.Listing {listing}. A test case that does not reach the fault.
[source, java]
----
@Test
public void testNull() {
    assertThrows(NullPointerException.class, ()  -> {
        findLast(null, 1);
    });
}
----

Infection::

Reaching the location of the fault is not the only condition that should be met to discover the fault. The test case should also produce an incorrect program state, that is it should _infect_ the program state with incorrect values. <<ripr-no-infection>> fails to do that. For this test case the element in the first position of the array is never inspected. The last occurrence of the element given as input is found at the last position of the array. The test case has the same behavior in the presence of the fault as is the program was correct. On the other hand, <<ripr-no-propagation>> do infect the state of the program. The first position is not checked in the faulty version which is not the case for the correct program.
+
[[ripr-no-infection, Listing {counter:listing}]]
.Listing {listing}. A test case that reaches the location of the fault but does not infect the program state.
[source, java]
----
@Test
public void testLastElement() {
    int[] array = {0, 1, 2};
    assertEquals(array.length - 1, findLast(array, 2));
}
----

Propagation::

<<ripr-no-propagation>> do infect the program but does not reveal the fault. A test case must reach the location of the fault, infect the program state but also _propagate_ the program state infection to the rest of the execution. <<<ripr-no-propagation>> produces an incorrect program state as the first position of the array is not inspected but returns the right result, so the error does not even reach the code of the test case.
+
[[ripr-no-propagation, Listing {counter:listing}]]
.Listing {listing}. A test case that reaches the fault, infects the program state but does not propagate the infection.
[source, java]
----
@Test
public void testNotFound() {
    assertEquals(-1, findLast(new int[]{0, 1, 2}, 4));
}
----

Revealability::

It is very hard, if not impossible, to create an oracle to observe the entire state of the program. That is why, for a test case to discover a fault, it must not only reach the location, infect the program state and propagate the infection to the rest of the execution, it also must also observe the right portion of the state and use a strong condition to verify the state. <<ripr-no-revealability>> reaches the fault, infects the program state, produces a wrong result that propagates to the code of the test case, but the oracle is not adequate. The condition of the result to be greater than zero is met by the incorrect result `2`, it is an example of a weak oracle and the test case fails to _reveal_ the fault. 
+
[[ripr-no-revealability, Listing {counter:listing}]]
.Listing {listing}. A test case that fails to reveal the fault, due to a weak oracle. The method returns a worng value `2` when the correct value is `0`. Both values meet the assertion.
[source, java]
----
@Test
public void testRepeated() {
    assertTrue(findLast( new int[]{0, 1, 0}, 0) >= 0);
}
----
+
Only the test case in <<ripr-test>> meets all the conditions to reveal the fault in the method. The method produces an incorrect value, `-1`, which is not the expected value `0`.
+
[[ripr-test, Listing {counter:listing}]]
.Listing {listing}. A test case able to reveal the fault.
[source, java]
----
@Test
public void testFirst() {
    assertEquals(0, findLast(new int[]{0, 1, 2}, 0));
}
----

