== Software Testing

According to Ammann and Offutt <<amman2017introduction>> _Software Testing_ is the process of evaluating a software by observing its execution to reveal the presence of faults or as Meyer says in his principles <<meyer2008seven>> _"`to test a program is to try to make it fail`"_. Per this definition, testing is a form of dynamic analysis, as it requires the execution of the program or system under test. The testing process is achieved with the design and application of test cases. 

In broad terms, a test case must provide the required _input_ and set the program in the desired state by _triggering specific behaviors_. It then, must _check the output_ of the program against expected results. Often, the output of the test execution is validated with an _oracle_, a predicate to tell whether the execution was successful or not. A test case may optionally include _prefix values_ which are inputs necessary to get the system ready for the input and _postfix values_ which are inputs needed to clean the environment after the execution of the test. Test cases may be _functional_, if it checks that a functionality is correct, or _non-functional_ if it is directed to evaluate properties like performance, security or even energy consumption. If a test case executes normally it is said to _pass_ otherwise, if the output is not correct we say that there is a _test failure_ and that the test _fails_.

A test case exercises the system in one specific scenario. So, only one test case is not enough to correctly verify the system. Therefore we always create a set of multiple test cases usually called _test suite_. 

=== Levels of testing

Testing can (and should) be done at different levels of abstractions <<amman2017introduction>> <<aniche-software>>:

Unit Testing:: It is the lowest level of testing and targets the code units of the program: procedures, functions or methods. The goal is to assess the _implementation_ of the software. For example, a unit test for a function that computes the factorial of a given number, would call the function with an input, say 5 and then check that the result is 120. 

Module Testing:: Units are often grouped in _modules_ that could be classes, packages or files. This level of testing verifies each module in isolation and evaluates how the component units interact with each other. For example, a test for a `Stack` class may create an instance of the class, push an element onto the stack, pop it out and finally check that the stack is empty.

NOTE: In Object-Oriented Programming a class is considered as both a unit and a module. Therefore the distinction between _unit tests_ and _module tests_ is not clear within this context. In fact, for most testers and developers both types of testing are known as _unit testing_ and it will be also the case for the rest of this text. 

Integration Testing:: The goal of this level of testing is to assess the interaction between modules and verify that they are communicating correctly. Integration tests assume that modules work correctly, that is the task of the previous testing levels. As an example, an integration test for an online store may check the interaction between the shopping cart module and the payment gateway by creating and validating an order, or it could also verify that data access abstractions communicate correctly with the database by creating the right queries and interpreting the results in the right way.

System Testing::  The goal of this level is to check whether the assembled system meets the specifications thus, It assumes that all subsystems work correctly, which is the task of the previous testing levels. Integration tests execute the system as a whole from beginning to end, that is why they are sometimes also referred as _end-to-end_ tests. An example of such tests in the context of an online store could verify that: a user can log in with the right credentials, she can create a shopping cart with a selection of products, she can validate the order, she receives an email with the invoice and then she is able to log out. Testing done from the graphical user interface is also a for of system testing.

Acceptance Testing:: This level checks that the finished product meets the needs of the users, that is, if the system does what they want. These tests should be elaborated from the user's perspective and may be used to evaluate the requirements. Like system tests, these also verify the product from beginning to end.

NOTE: There might be other, even finer-grained, classifications for tests and they all tend to be blurred sometimes ambiguous. Classifying one test case as unit, module, integration or system is not crucial. The most important message here is that testing should be carried at different levels, for each unit, whatever they may be, then how these units interact and then how the entire system behaves.

Each level of testing has its own level of complexity. Unit tests tend to be easier to define and faster to execute. System tests are more complex and slower. Inputs for unit tests may not be completely realistic compared to the inputs that shall appear in production and they are not generally able to find faults arising from the interaction of modules. System tests tend to be more realistic  Also finding an implementation fault from a system test may be even impractical.

Automating the execution of tests at all levels is key for modern software development practices. Manual tests are tedious, take a lot of time and are prone to errors. Automated test execution allows having more tests and therefore more scenarios are explored. As Meyer explains in his fourth principle,<<meyer2008seven>> it is not feasible to manually check the output of hundreds of tests. Automated tests help producing a faster, almost immediate feedback during development on whether certain types of faults are being inserted. In this way they enable efficient CI/CD processes which facilitate the integration of the work of development teams. Automated tests help also preventing _regressions_ that is, the reinsertion of an already solved fault. This is specially true if we adhere to Meyer's third principle "`Any failed execution must yield a test case, to remain a permanent part of the project’s test suite`" <<meyer2008seven>>.

The accepted practice nowadays is to have most tests automated, while keeping only a small amount of manual tests, mainly as acceptance tests. Most organizations have a large number of unit tests, which are easier to write, faster to execute and easier to understand and less system tests which are more complex, expensive and harder to maintain. This is reflected on what is known as the _testing pyramid_ <<testing-pyramid>>.

[#testing-pyramid.text-center]
.The testing pyramid.
image::testing-pyramid.png[Testing pyramid, 600]

So, as we move in the pyramid from unit tests, which directly assess the implementation of the system,  up to acceptance tests which check the compliance of the system with the users' needs, we mode from verification to validation.

=== Test automation frameworks: JUnit

Test automation allows having more tests to explore more execution scenarios, provides a faster feedback and facilitates integration processes. It is achieved with the help of _test automation frameworks_ or _testing frameworks_.

A testing framework is a set of tools and libraries providing mechanisms to define or specify test cases and execute them. One of the most famous alternatives at the moment is _JUnit_, initially developed by Kent Beck back in 1997. JUnit has become a sort of _de-facto_ standard for Java projects and has inspired the creation of similar frameworks for other languages and platforms which are informally called as the _xUnit_ family.
Despite hading "`unit`" in the name and being widely used for unit testing, the framework can be use to implement all sort of automated tests.

NOTE: At the moment of writing this material the latest stable version of JUnit is 5.6.2. This version will be used for all code examples.

Suppose we have a `Stack` class that implements a _LIFO_ (Last In First Out) data structure. The class has a `void` method `push` to insert an element onto the stack and `pop` that removes the element on top of the stack and returns it. A typical unit test for this class written with the help of JUnit would look like the code shown in <<junit-example>>.

[[junit-example, Listing {counter:listing}]]
.Listing {listing}. A typical unit test written with JUnit.
[source,java]
----
class StackTest {
    @Test
    public void testPushPop() {
        int original = 1;
        Stack stack = new Stack();
        stack.push(original);
        int onTop = stack.pop();
        assertSame(item, onTop, "Element on top of the stack should be " + original);
    }
}
----

Test cases in JUnit are implemented inside _test classes_. These classes declare _test methods_ which contain the main code for the test cases. These test methods are identified with the `@Test` annotation. In <<junit-example>> the first four lines of `testPushPop` provide the input values of the test case and set the instance of `Stack` in the required state: an element has been pushed and then popped from the stack. 

The last line uses an oracle to verify that the element obtained from the stack was the same that was pushed in the first place. This type of oracle is known as an _assertion_. It evaluates a given condition and if the condition is false an `AssertionError` is thrown. It also includes a message to use as output in the case the assertion fails. In the absence of any assertion in the code, JUnit tests have an implicit oracle that checks if unexpected errors occur, that is, if an unexpected exception is thrown.

JUnit provides a set of utility methods implementing different assertions such as: `assetEquals` that checks if two given objects are equal, `assertNotEqual`, the contrary, `assertNull` which verifies if a given value is `null` or not, `assertSame` used in the example to verify if two objects are the same and many more.

In some scenarios, a test case should verify whether an operation with the wrong input signals the right error. <<junit-throw>> shows how to achieve this. The test verifies that invoking `pop` in an empty `Stack` should throw an `IllegalOperationException`.

[[junit-throw, Listing {counter:listing}]]
.Listing {listing}. Verifying the correct error with JUnit.
[source,java]
----
@Test
public void testErrorPopEmptyStack() {
    assertThrows(IllegalOperationException.class, () -> {
        new Stack().pop();
    });
}
----

While the assertions included in JUnit cover a wide spectrum of scenarios, libraries like http://hamcrest.org/JavaHamcrest/[Hamcrest] and https://joel-costigliola.github.io/assertj/[AssertJ] help creating more expressive and higher level assertions.

A test case in JUnit could be more than a single test method, it may include other methods supporting the test execution. For example, methods annotated with `@BeforeEach` and `@AfterEach` will be executed before and after each identified test cases in the same test class respectively. These are helpful to set prefix and postfix test inputs.

JUnit includes many additional functionalities to facilitate the creation of tests, such as parameterized tests, special oracles to verify the performance of the code and even the dynamic creation of tests.

It is important to add information that helps identifying the fault in the event of a test failure. In JUnit, and any other testing framework, a common practice to try to achieve this is to use descriptive names for test methods and set detailed messages for assertions. However, there are many other characteristics that good test cases must have in practice.

=== Test Antipatterns / Smells

Automated test cases are code, _test code_, and like the _application code_ they should be maintained and we should care about their quality. Poorly written test cases bring no value to the development process. They negatively impact the fault detection capabilities of a test suite. They are also hard to understand and hard to leverage when identifying faults.

As summarized in <<meszaros2003test>> automated test cases should be *concise* and *clear*: brief yet comprehensive and easy to understand, *self-checking*: they should report results without human intervention, *repeatable*, *robust* and *independent*: it should be possible to run them consecutive times without human intervention and they should always produce the same results whether their are run in isolation or with other tests. Tests should also be *efficient*: they should be run in a reasonable amount of time and should be *maintainable*: that is, they must be easy to modify and extend even when the system under test changes. Also, with respect to the application and the requirements, tests must be *sufficient* so all requirements of the system are verified, *necessary* so that everything in each test contributes to the specification of the desired behavior, with no redundancy and unneeded artifacts, each test should be *specific* so tests failures point to the specific fault, and *traceable* so that it can be easily mapped to the parts of the application code it verifies and the part of the specification it has been derived from. 

Along the years, the testing community has identified bad practices, _smells_ that deviate from the principles mentioned above and impact the quality of tests. Garousi and Küçük <<garousi2018smells>> reviewed the scientific and industry literature on the subject and were able to identify 179 different test smells. It is important to notice that test smells are not bugs but affect the tests by lowering their efficiency, maintainability, readability, comprehension and their ability to find faults. This section presents and exemplifies some of these test smells.

Manual intervention:: Happens when the person running the test case must do something manually before the test is run, during the execution or should manually verify the results. This goes in detriment of test automation.

Testing Happy Path only:: Tests only verify the common scenario, and never check boundaries or input values that should result in exceptions. Most of the time developers program with the happy path / normal situation in mind and it is most likely that this scenario will work. Therefore testing the happy path only have lower chances to catch a bug. The test case in <<junit-example>> tests only the most expected scenario or happy path. We need to add test cases like <<junit-throw>> where we explore extreme scenarios like a pop on an empty stack or when a null element is pushed or if if there is a point at which we can push no more elements to the stack.

Test logic in production code:: The application code put into production contains logic that should be exercised only during test execution. This logic has been put there only to support testing, for example, to help tests gain access to the internal state of the application. It also may happen that part of the production logic can not be executed in testing. This makes the system behaves differently in production and testing. An example is shown in <<test-logic-in-production>>.

[[test-logic-in-production, Listing {counter:listing}]]
.Listing {listing}. Example of test logic in production code.
[source,java]
----
...
if (System.getProperty("env", "prod").equals("test")) {
    return new User('Jane Doe', 'janedoe@example.com'); //<1>
}
else {
    User user = new User(Request.getParam(login), Request.getParam(name));
    validateUser(user);
    return user;
}
...
----
. <1> This line makes the code return a wired values to use in production.

Another example of this type of smell is when a class does not require an implementation of `equals` and we do need it just for testing purposes. This is known as _equality pollution_. The application code is filled with unnecessary `equals` methods, whose logic may actually go against the requirements. 

In general, all forms of this test smell make the application code more complex and introduces maintainability issues.

A way to solve this smell, is to use _dependency injection_. The code that has to work differently in production and tests can be moved onto a dependency that can be exchanged without affecting the application logic. In case of equality pollution we could use an _equality comparer_, that is, a class that checks if two objects are equals per our needs.

Eager test:: Also known as *The Test It All* or *Split Personality*. It is a single test that verifies too many functionalities. <<eager-roulette>> shows an example of this test smell. When such a test fails, it is hard to tell which is the actual functionality that contains the fault. The solution is to separate all verifications into different test cases.

[[eager-roulette, Listing {counter:listing}]]
.Listing {listing}. An example of a test that tries to test too much in the same test case (Eager Test) and it is also hard to know the fault in the presence of a test failure. Taken from <<xunitpatterns-assertion>>.
[source,java]
----
@Test
public void testFlightMileage_asKm2() throws Exception {
    // setup fixture
    // exercise contructor
    Flight newFlight = new Flight(validFlightNumber);
    // verify constructed object
    assertEquals(validFlightNumber, newFlight.number);
    assertEquals("", newFlight.airlineCode);
    assertNull(newFlight.airline);
    // setup mileage
    newFlight.setMileage(1122);
    // exercise mileage translater
    int actualKilometres = newFlight.getMileageAsKm();    
    // verify results
    int expectedKilometres = 1810;
    assertEquals( expectedKilometres, actualKilometres);
    // now try it with a canceled flight:
    newFlight.cancel();
    try {
        newFlight.getMileageAsKm();
        fail("Expected exception");
    } catch (InvalidRequestException e) {
        assertEquals( "Cannot get cancelled flight mileage", e.getMessage());
    }
}
----

Assertion roulette:: Appears when it is hard to tell which of the many assertions of  a test method produced the test failure. This makes harder to diagnose the actual fault. Eager tests tend to also produce assertion roulettes as can be seen in <<eager-roulette>>. This smell also occurs when assertions do not have any message, as seen in <<no-message-assertion>>. To solve this smell we should refactor the test code and add a descriptive message to all assertions.

[[no-message-assertion, Listing {counter:listing}]]
.Listing {listing}. Example of a test case with several assertions with no message. In the case of a test failure it is hard to know which assertion failed and to diagnose the fault. Taken from <<xunitpatterns-assertion>>
[source,java]
----
@Test
public void testInvoice_addLineItem7() {
    LineItem expItem = new LineItem(inv, product, QUANTITY);
    // Exercise
    inv.addItemQuantity(product, QUANTITY);
    // Verify
    List lineItems = inv.getLineItems();
    LineItem actual = (LineItem)lineItems.get(0);
    assertEquals(expItem.getInv(), actual.getInv());
    assertEquals(expItem.getProd(), actual.getProd());
    assertEquals(expItem.getQuantity(), actual.getQuantity());
}
----

The Free Ride:: Also known as *Piggyback* and closely related to the two previous test smells. In this smell, rather than write a new test case method to test another feature or functionality, testers add new assertions to verify other functionalities. It can lead to eager tests and assertion roulettes. As with this two other smells, piggybacking makes it hard to diagnose the fault. <<piggybacking>> shows an actual example of this smell from the Apache Commons Lang project.

[[piggybacking, Listing {counter:listing}]]
.Listing {listing}. Actual example of the piggybacking test smell. Code can be consulted in the https://github.com/apache/commons-lang/blob/649dedbbe8b6ab61fb3c4792c86b7e0af7ec4a73/src/test/java/org/apache/commons/lang3/ArrayUtilsRemoveMultipleTest.java#L34[Apache Commons Lang code repository]. This is also an example of an eager test and assertion roulette.
[source,java]
----
@Test
public void testRemoveAllBooleanArray() {
    boolean[] array;

    array = ArrayUtils.removeAll(new boolean[] { true }, 0);
    assertArrayEquals(ArrayUtils.EMPTY_BOOLEAN_ARRAY, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 0);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 1);
    assertArrayEquals(new boolean[]{true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true }, 1);
    assertArrayEquals(new boolean[]{true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 0, 1);
    assertArrayEquals(ArrayUtils.EMPTY_BOOLEAN_ARRAY, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 0, 1);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 0, 2);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 1, 2);
    assertArrayEquals(new boolean[]{true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 0, 2, 4);
    assertArrayEquals(new boolean[]{false, false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 1, 3);
    assertArrayEquals(new boolean[]{true, true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 1, 3, 4);
    assertArrayEquals(new boolean[]{true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 0, 2, 4, 6);
    assertArrayEquals(new boolean[]{false, false, false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 1, 3, 5);
    assertArrayEquals(new boolean[]{true, true, true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 0, 1, 2);
    assertArrayEquals(new boolean[]{false, true, false, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());
}
----

Interacting Tests:: Tests that depend on each other in some way. It may happen when one test depends of the outcome of another, for example, as a result of a test, a file is created which is used to execute another test. In this way a test may fail for reasons other than a fault in the behavior it is verifying.

The Local Hero:: A test case depends on something specific to the development environment. It passes in a matching environment but fails under any other conditions. This may happen when tests depend on the existence of specific services or even machine features. Such assumptions should always be avoided.

Conditional test logic:: Also known as *Guarded Test*. Consists in a test that contains code that may or may not be executed. It makes tests more complicated than actually needed and therefore less readable and maintainable. It usually appears with the use of control structures within a test method. <<conditional-logic>> shows an example.

[[conditional-logic,Listing {counter:listing}]]
.Listing {listing}. An example of conditional logic in a test. In this case, if the element is not returned by the iterator, the test executes without failing.
[source, java]
----
 //  verify Vancouver is in the list:
    actual = null;
    i = flightsFromCalgary.iterator();
    while (i.hasNext()) {
        FlightDto flightDto = (FlightDto) i.next();
        if (flightDto.getFlightNumber().equals( expectedCalgaryToVan.getFlightNumber())) //<1>
        {
            actual = flightDto;
            assertEquals("Flight from Calgary to Vancouver", expectedCalgaryToVan, flightDto);
            break;
        }
    }
}
----
. <1> Checks the presence of an element. If the element is not there, then the test executes and does not fail.

Fragile test:: A test that fails to compile or run when the system under test is changed in ways that do not affect the part the test is exercising. These tests increase the cost of maintenance. There are many causes for this smell, so code should be carefully inspected and refactored.



Get really clever and use random numbers in your tests::

Testing private methods:: Also known as  *X-Ray Specs*.  