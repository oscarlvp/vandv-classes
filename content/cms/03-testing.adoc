== Software Testing

According to Ammann and Offutt <<ammann2016introduction>> _Software Testing_ is the process of evaluating a software by observing its execution to reveal the presence of faults. Or, as Meyer says in his principles <<meyer2008seven>> _"`to test a program is to try to make it fail`"_. Per this definition, testing is a form of dynamic analysis: it requires the execution of the program or system under test.  

Testing is achieved through the design and application of test cases. In broad terms, a test case is a set of steps and values that must provide the required _input_ and set the program in the desired state by _triggering specific behaviors_. It then, must _check the output_ of the program against expected results. Often, the output of the test case execution is validated with an _oracle_, _i.e._ a predicate to tell whether the execution was successful or not. A test case may optionally include _prefix values_ which are inputs necessary to get the system ready for the input and _postfix values_ which are inputs needed to clean the environment after the execution of the test. Test cases may be _functional_, if they check that a functionality is correct, or _non-functional_ if it is directed to evaluate properties like performance, security or even energy consumption. If a test case executes normally it is said to _pass_ otherwise, if the output is not correct, we say that there is a _test failure_ and that the test _fails_.

A test case exercises the system in one specific scenario. So, only one test case is not enough to correctly verify the system. Therefore we always need tp create a set of multiple test cases. This set is usually called _test suite_. 

=== Levels of testing

Testing can (and should) be done at different levels of abstractions <<amman2017introduction>> <<aniche-software>>:

Unit Testing:: It is the lowest level of testing and targets the code units of the program: procedures, functions or methods. The goal is to assess the _implementation_ of the software. For example, a unit test for a function that computes the factorial of a given number, would call the function with an input, say 5 and then check that the result is 120. 

Module Testing:: Units are often grouped in _modules_ that could be classes, packages or files. This level of testing verifies each module in isolation and evaluates how the component units interact with each other. For example, a test for a `Stack` class may create an instance of the class, push an element onto the stack, pop it out and finally check that the stack is empty.

NOTE: In Object-Oriented Programming a class is at the same time a unit and a module. Therefore the distinction between _unit tests_ and _module tests_ is not clear within this context. In fact, for most testers and developers both types of testing are known as _unit testing_ and it will be also the case for the rest of this text. 

Integration Testing:: The goal of this level of testing is to assess the interaction between modules and verify that they are communicating correctly. Integration tests assume that modules work correctly. Modules should be verified by the previous testing levels. As an example, an integration test for an online store may check the interaction between the shopping cart module and the payment gateway by creating and validating an order, or it could also verify that data access abstractions communicate correctly with the database by creating the right queries and interpreting the results in the right way.

System Testing::  The goal of this level is to check whether the assembled system meets the specifications. Thus, it assumes that all subsystems work correctly, which is the task of the previous testing levels. Integration tests execute the system as a whole from beginning to end, that is why they are sometimes also referred as _end-to-end_ tests. An example of such tests in the context of an online store could verify that: a user can log in with the right credentials, she can create a shopping cart with a selection of products, she can validate the order, she receives an email with the invoice and then she is able to log out. Testing done from the graphical user interface is also a form of system testing.

Acceptance Testing:: This level checks that the finished product meets the needs of the users, that is, if the system does what users want. These tests should be elaborated from the user's perspective and may be used to evaluate the requirements. Like system tests, these also verify the product from beginning to end.

NOTE: There might be other, even finer-grained, classifications for tests and they all tend to be blurred and even ambiguous. Classifying one test case as unit, module, integration or system is not crucial. The most important message here is that testing should be carried at different levels, for each unit, whatever they may be, then how these units interact and then how the entire system behaves.

Each level of testing has its own level of complexity. Unit tests tend to be easier to define and faster to execute. System tests are more complex and slower. Inputs for unit tests may not be completely realistic compared to the inputs that shall appear in production and they are not generally able to find faults arising from the interaction of modules. System tests tend to be more realistic. ON the oder hand, finding an implementation fault from a system test may be even impractical.

Automating the execution of tests at all levels is key for modern software development practices. Manual tests are tedious, take a lot of time and are prone to errors. Automated test execution allows having more tests and therefore more scenarios are explored. As Meyer explains in his fourth principle,<<meyer2008seven>> it is not feasible to manually check the output of hundreds of tests. Automated tests help producing a faster, almost immediate feedback during development on whether certain types of faults are being inserted. In this way they enable efficient CI/CD processes which facilitate the integration of the work of development teams. Automated tests help also preventing _regressions_ that is, the reinsertion of an already solved fault. This is specially true if we adhere to Meyer's third principle "`Any failed execution must yield a test case, to remain a permanent part of the project’s test suite`" <<meyer2008seven>>.

The accepted practice nowadays is to have most tests automated, while keeping only a small amount of manual tests, mainly as acceptance tests. Most organizations have a large number of unit tests, which are easier to write, faster to execute and easier to understand. Organizations tend to have less system tests which are more complex, expensive and harder to maintain. This is reflected on what is known as the _testing pyramid_ <<testing-pyramid>>.

[#testing-pyramid.text-center]
.The testing pyramid.
image::testing-pyramid.png[Testing pyramid, 600]

So, as we move up in the pyramid from unit tests, which directly assess the implementation of the system, to acceptance tests which check the compliance of the system with the users' needs, we move from the area of verification to the area of validation.

=== Test automation frameworks: JUnit

Test automation allows having more tests to explore more execution scenarios, provides a faster feedback and facilitates integration processes. It is achieved with the help of _test automation frameworks_ or _testing frameworks_.

A testing framework is a set of tools and libraries providing mechanisms to define or specify test cases and execute them. One of the most famous alternatives at the moment is _JUnit_, initially developed by Kent Beck in 1997. JUnit has become a sort of _de-facto_ standard for Java projects and has inspired the creation of similar frameworks for other languages and platforms which are informally called as the _xUnit_ family.
Despite having "`unit`" in the name and being widely used for unit testing, the framework can be use to implement all sort of automated tests.

NOTE: At the moment of writing this material the latest stable version of JUnit is 5.6.2. This version will be used for all code examples.

Suppose we have the `BoundedStack` class, shown in <<stack-example>>, that implements a _LIFO_ (Last In First Out) data structure with a fixed capacity. The class has a `void` method `push` to insert an element onto the stack and `pop` that removes the element on top of the stack and returns it. 

[[stack-example, Listing {counter:listing}]]
.Listing {listing}. The `BoundedStack` class represents  a _LIFO_ data structure with maximum capacity.
[source, java]
----
public class BoundedStack {
    private int[] elements;
    private int count;

    public BoundedStack(int capacity) {
        elements = new int[capacity];
        count = 0;
    }

    public void push(int item) {
        if(count == elements.length) {
            throw new IllegalStateException();
        }
        elements[count] = item;
        count = count + 1;
    }

    public int pop() {
        if(count == 0) {
            throw new NoSuchElementException();
        }
        count = count - 1;
        return elements[count];
    }

    public int size() {
        return count;
    }

    public int capacity() {
        return elements.length;
    }
}
----

A typical unit test for this class written with the help of JUnit would look like the code shown in <<junit-example>>.

[[junit-example, Listing {counter:listing}]]
.Listing {listing}. A typical unit test written with JUnit.
[source,java]
----
class BoundedStackTest {
    @Test
    public void testPushPop() {
        int original = 1;
        BoundedStack stack = new BoundedStack(10);
        stack.push(original);
        int onTop = stack.pop();
        assertEquals(original, onTop, "Element on top of the stack should be " + original);
    }
}
----

Test cases in JUnit are implemented inside _test classes_. These classes declare _test methods_ which contain the main code for the test cases. These test methods are identified with the `@Test` annotation. In <<junit-example>> the first four lines of `testPushPop` provide the input values of the test case and set the instance of `BoundedStack` in the required state: an element has been pushed and then popped from the stack. 

The last line uses an oracle to verify that the element obtained from the stack was the same that was pushed in the first place. This type of oracle is known as an _assertion_. It evaluates a given condition and if the condition is false an `AssertionError` is thrown. It also includes a message to use as output in the case the assertion fails. In the absence of any assertion in the code, JUnit tests have an implicit oracle that checks if unexpected errors occur, that is, if an unexpected exception is thrown.

JUnit provides a set of utility methods implementing different assertions such as: `assetEquals` that checks if two given objects are equal and it is used in the example, `assertNotEqual`, the contrary, `assertNull` which verifies if a given value is `null`, `assertSame` to verify if two objects are the same and many more.

In some scenarios, a test case should verify whether an operation with the wrong input signals the right error. <<junit-throw>> shows how to achieve this. The test verifies that invoking `pop` in an empty `Stack` should throw an `IllegalStateException`.

[[junit-throw, Listing {counter:listing}]]
.Listing {listing}. Verifying the correct error with JUnit.
[source,java]
----
@Test
public void testErrorPopEmptyStack() {
    assertThrows(NoSuchElementException.class, () -> {
        new BoundedStack(1).pop();
    });
}
----

While the assertions included in JUnit cover a wide spectrum of scenarios, libraries like http://hamcrest.org/JavaHamcrest/[Hamcrest] and https://joel-costigliola.github.io/assertj/[AssertJ] help creating more expressive and higher level assertions.

A test case in JUnit could be more than a single test method, it may include other methods supporting the test execution. For example, methods annotated with `@BeforeEach` and `@AfterEach` will be executed before and after each identified test cases in the same test class respectively. These are helpful to set prefix and postfix test inputs.

JUnit includes many additional functionalities to facilitate the creation of tests, such as parameterized tests, special oracles to verify the performance of the code and even the dynamic creation of tests.

It is important to add information that helps identifying the fault in the event of a test failure. In JUnit, and any other testing framework, a common practice to try to achieve this is to use descriptive names for test methods and set detailed messages for assertions. However, there are many other characteristics that good test cases must have in practice.

=== Best practices and antipatterns in testing

Automated test cases are code, _test code_, and like their counter part, the _application code_ they should be maintained and we should care about their quality. Poorly written test cases bring no value to the development process. They negatively impact the fault detection capabilities of a test suite. They are also hard to understand and hard to leverage to correctly identify faults.

As summarized in <<meszaros2003test>> automated test cases should be *concise* and *clear*: brief, yet comprehensive and easy to understand, *self-checking*: they should report results without human intervention, *repeatable*, *robust* and *independent*: it should be possible to run them consecutive times without human intervention and they should always produce the same results whether their are run in isolation or with other tests. Tests should also be *efficient*: they should run in a reasonable amount of time and should be *maintainable*: that is, they must be easy to modify and extend even when the system under test changes. Also, with respect to the application and the requirements, tests must be *sufficient* so all requirements of the system are verified, *necessary* so that everything inside each test contributes to the specification of the desired behavior, with no redundancy and unneeded artifacts, each test should be *specific* so tests failures point to the specific fault and the broken functionality, and *traceable* so that it can be easily mapped to the parts of the application code it verifies and the part of the specification it has been derived from. 

==== Test smells

Along the years, the testing community has identified bad practices, _smells_ that deviate from the principles mentioned above and have a negative impact in the quality of tests. Garousi and Küçük <<garousi2018smells>> reviewed the scientific and industry literature on the subject and were able to identify 179 different test smells. It is important to notice that test smells are not bugs but affect the tests by lowering their efficiency, maintainability, readability, comprehension and their ability to find faults. This section presents and exemplifies some of these test smells.

Manual intervention:: Happens when the person running a test case must do something manually before the test is run, during the execution of the test or she should manually verify the results. This practice undermines test automation.

Testing Happy Path only:: These tests verify only the common scenario and never check boundaries or input values that should result in exceptions. Most of the time developers write code with the happy path / normal situation in mind and it is most likely that this scenario will work. Therefore, testing only the happy path have lower chances to catch a bug. The test case in <<junit-example>> tests only the most expected scenario or happy path. We need to add test cases like <<junit-throw>> where we explore extreme scenarios like a pop on an empty stack or when a null element is pushed, or if there is a point at which we can push no more elements to the stack.

Test logic in production code:: The application code deployed in production contains logic that should be exercised only during test execution. This logic is there only to support testing, for example, to help tests gain access to the internal state of the application. It also may happen that part of the production logic can not be executed in testing. This makes the system behaves differently in production and testing. An example is shown in <<test-logic-in-production>>.

[[test-logic-in-production, Listing {counter:listing}]]
.Listing {listing}. Example of test logic in production code.
[source,java]
----
...
if (System.getProperty("env", "prod").equals("test")) {
    return new User('Jane Doe', 'janedoe@example.com'); //<1>
}
else {
    User user = new User(Request.getParam(login), Request.getParam(name));
    validateUser(user);
    return user;
}
...
----
<1> This line makes the code return a wired values to use in production.

Another example of this type of smell is when a class does not require an implementation of `equals` and we do need it just for testing purposes. This is known as _equality pollution_. The application code is filled with unnecessary `equals` methods, whose logic may actually go against the requirements. 

In general, all forms of this test smell make the application code more complex and introduces maintainability issues.

A way to solve this smell, is to use _dependency injection_. The code that has to work differently in production and tests can be moved onto a dependency that can be exchanged without affecting the application logic. In case of equality pollution we could use an _equality comparer_, that is, a class that checks if two objects are equals per our needs.

Eager test:: Also known as *Test It All* or *Split Personality*. It is a single test that verifies too many functionalities. <<eager-roulette>> shows an example of this test smell. When such a test fails, it is hard to tell which code produced the failure. The solution is to separate all verifications into different test cases.

[[eager-roulette, Listing {counter:listing}]]
.Listing {listing}. An example of a test that tries to test too much in the same test case (Eager Test) and it is also hard to know the fault in the presence of a test failure. Taken from <<xunitpatterns-assertion>>.
[source,java]
----
@Test
public void testFlightMileage_asKm2() throws Exception {
    // setup fixture
    // exercise contructor
    Flight newFlight = new Flight(validFlightNumber);
    // verify constructed object
    assertEquals(validFlightNumber, newFlight.number);
    assertEquals("", newFlight.airlineCode);
    assertNull(newFlight.airline);
    // setup mileage
    newFlight.setMileage(1122);
    // exercise mileage translater
    int actualKilometres = newFlight.getMileageAsKm();    
    // verify results
    int expectedKilometres = 1810;
    assertEquals( expectedKilometres, actualKilometres);
    // now try it with a canceled flight:
    newFlight.cancel();
    try {
        newFlight.getMileageAsKm();
        fail("Expected exception");
    } catch (InvalidRequestException e) {
        assertEquals( "Cannot get cancelled flight mileage", e.getMessage());
    }
}
----

Assertion roulette:: Appears when it is hard to tell which of the many assertions of a test method produced the test failure. This makes harder to diagnose the actual fault. Eager tests tend to also produce assertion roulettes as can be seen in <<eager-roulette>>. This smell also occurs when assertions do not have any message, as seen in <<no-message-assertion>>. To solve this smell we should refactor the test code and add a descriptive message to all assertions.

[[no-message-assertion, Listing {counter:listing}]]
.Listing {listing}. Example of a test case with several assertions with no message. In the case of a test failure it is hard to know which assertion failed and to diagnose the fault. Taken from <<xunitpatterns-assertion>>
[source,java]
----
@Test
public void testInvoice_addLineItem7() {
    LineItem expItem = new LineItem(inv, product, QUANTITY);
    // Exercise
    inv.addItemQuantity(product, QUANTITY);
    // Verify
    List lineItems = inv.getLineItems();
    LineItem actual = (LineItem)lineItems.get(0);
    assertEquals(expItem.getInv(), actual.getInv());
    assertEquals(expItem.getProd(), actual.getProd());
    assertEquals(expItem.getQuantity(), actual.getQuantity());
}
----

The Free Ride:: Also known as *Piggyback* and closely related to the two previous test smells. In this smell, rather than write a new test case method to test another feature or functionality, testers add new assertions to verify other functionalities. It can lead to eager tests and assertion roulettes. As with those two other smells, piggybacking makes it hard to diagnose the fault. <<piggybacking>> shows an actual example of this smell from the Apache Commons Lang project.

[[piggybacking, Listing {counter:listing}]]
.Listing {listing}. Actual example of the piggybacking test smell. Code can be consulted in the https://github.com/apache/commons-lang/blob/649dedbbe8b6ab61fb3c4792c86b7e0af7ec4a73/src/test/java/org/apache/commons/lang3/ArrayUtilsRemoveMultipleTest.java#L34[Apache Commons Lang code repository]. This is also an example of an eager test and assertion roulette.
[source,java]
----
@Test
public void testRemoveAllBooleanArray() {
    boolean[] array;

    array = ArrayUtils.removeAll(new boolean[] { true }, 0);
    assertArrayEquals(ArrayUtils.EMPTY_BOOLEAN_ARRAY, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 0);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 1);
    assertArrayEquals(new boolean[]{true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true }, 1);
    assertArrayEquals(new boolean[]{true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false }, 0, 1);
    assertArrayEquals(ArrayUtils.EMPTY_BOOLEAN_ARRAY, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 0, 1);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 0, 2);
    assertArrayEquals(new boolean[]{false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, false }, 1, 2);
    assertArrayEquals(new boolean[]{true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 0, 2, 4);
    assertArrayEquals(new boolean[]{false, false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 1, 3);
    assertArrayEquals(new boolean[]{true, true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true }, 1, 3, 4);
    assertArrayEquals(new boolean[]{true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 0, 2, 4, 6);
    assertArrayEquals(new boolean[]{false, false, false}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 1, 3, 5);
    assertArrayEquals(new boolean[]{true, true, true, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());

    array = ArrayUtils.removeAll(new boolean[] { true, false, true, false, true, false, true }, 0, 1, 2);
    assertArrayEquals(new boolean[]{false, true, false, true}, array);
    assertEquals(Boolean.TYPE, array.getClass().getComponentType());
}
----

Interacting Tests:: Tests that depend on each other in some way. It may happen when one test depends of the outcome of another, for example, as a result of a test, a file is created which is used to execute another test. In this way a test may fail for reasons other than a fault in the behavior it is verifying.

The Local Hero:: A test case depends on something specific to the development environment. It passes in a matching environment but fails under any other conditions. This may happen when tests depend on the existence of specific services or even machine features. Such assumptions should always be avoided.

Conditional test logic:: Also known as *Guarded Test*. Consists in a test that contains code that may or may not be executed. It makes tests more complicated than actually needed and therefore less readable and maintainable. It usually appears with the use of control structures within a test method. <<conditional-logic>> shows an example.

[[conditional-logic,Listing {counter:listing}]]
.Listing {listing}. An example of conditional logic in a test. In this case, if the element is not returned by the iterator, the test executes without failing.
[source, java]
----
 //  verify Vancouver is in the list:
    actual = null;
    i = flightsFromCalgary.iterator();
    while (i.hasNext()) {
        FlightDto flightDto = (FlightDto) i.next();
        if (flightDto.getFlightNumber().equals( expectedCalgaryToVan.getFlightNumber())) //<1>
        {
            actual = flightDto;
            assertEquals("Flight from Calgary to Vancouver", expectedCalgaryToVan, flightDto);
            break;
        }
    }
}
----
<1> Checks the presence of an element. If the element is not there, then the test executes and does not fail.

Fragile test:: A test that fails to compile or run when the system under test is changed in ways that do not affect the part the test is exercising. These tests increase the cost of maintenance. There are many causes for this smell, so code should be carefully inspected and refactored.

Erratic tests:: Also known as *Flaky Tests*. It is a test that behave erratically under the same conditions, sometimes it fails and sometimes it does not. These tests undermine the trust developers have on their test suites. It is hard to know whether the failure is due to an actual fault or not. There are many reasons this could happen, for example: the already mentioned *Interacting Tests* smell, incorrect handling of the resources the test should use, and any type of non-determinism in tests coming from race conditions, synchronization, concurrency, time-outs and randomly generated data. Erratic or flaky tests are more common in higher level testing such as integration or system tests. They are a true plague for companies that develop big systems. As an example, Google has reported that nearly 1.5% of their tests behave erratically  <<micco2016flaky>>. 

Get really clever and use random numbers in your tests:: Using randomly generated data in tests cases is not necessarily a bad idea. Random tests can discover cases that developers have missed. However, random data has to be carefully managed to avoid creating *Erratic Tests* and to ensure that tests can be *repeatable*. Some of the key actions to consider are to use pseudo-random numbers and to store or log the seed used to generate the data and to log the data used in case of a failure. With this information, the test can be repeated later to diagnose the fault.

Testing private methods:: Also known as  *X-Ray Specs*. Tests should verify results, not the implementation. Results are most likely to remain the same even when the implementation changes. Results usually come from the specification and the implementation should match the specification. Private  methods are implementation artifacts hidden from external users. Verifying results should only involve the public API without knowing the internals of a module or class. Also, trying to test private methods, requires a non-trivial plumbing that would make tests more complicated. So, tests should not directly target private methods, but they _must assess their effects_ through the public API.

==== Real examples of good testing practices

The previous sections presented the features good tests should have and described a selection of common antipatterns in testing. This section presents examples of good testing practices in Apache Commons Math, a popular open-source Java library.

<<testing-exceptional-case>> shows a good example on how to handle the verification of exceptional cases and avoiding to test only the happy path. In this example, developers do two things. First, they annotate the expected exception in the test with `@Test(expected=...)`. With this, JUnit will verify that an exception is thrown and that it should be of the right type. Then, they used the special assertion `fail` to mark the part of the test code that should not be executed due to the exception. In this way they ensure that the test will not silently pass in the event of any fault and even if they do not annotate the test with the exception. The original code can be consulted https://github.com/apache/commons-math/blob/eb57d6d457002a0bb5336d789a3381a24599affe/src/test/java/org/apache/commons/math4/filter/KalmanFilterTest.java#L43
[here]. Note that this uses JUnit 4. In JUnit 5, `assertThrows` is preferred. 

[[testing-exceptional-case, Listing {counter:listing}]]
.Listing {listing}. A test case, testing the exceptional case, notice the use of `fail` to avoid finishing the test silently.
[source, java]
----
// In org.apache.commons.math3.filter.KalmanFilterTest
@Test(expected=MatrixDimensionMismatchException.class) //<1>
public void testTransitionMeasurementMatrixMismatch() {
    // A and H matrix do not match in dimensions
    // A = [ 1 ]
    RealMatrix A = new Array2DRowRealMatrix(new double[] { 1d });
    // no control input
    RealMatrix B = null;
    // H = [ 1 1 ]
    RealMatrix H = new Array2DRowRealMatrix(new double[] { 1d, 1d });
    // Q = [ 0 ]
    RealMatrix Q = new Array2DRowRealMatrix(new double[] { 0 });
    // R = [ 0 ]
    RealMatrix R = new Array2DRowRealMatrix(new double[] { 0 });

    ProcessModel pm
        = new DefaultProcessModel(A, B, Q,
                                    new ArrayRealVector(new double[] { 0 }), null);
    MeasurementModel mm = new DefaultMeasurementModel(H, R);
    new KalmanFilter(pm, mm);
    Assert.fail("transition and measurement matrix should not be compatible"); //<2>
}
----
<1> Annotation with an assertion to indicate that a `MatrixDimensionMismatchException` should be thrown.
<2> This line must not be executed, if the exception is properly thrown. This is therefore a safeguard ensuring that the test should fail in case this line is executed.

<<random-data>> shows a test case using random data. Developers fixed the seed to generate the random numbers. It could be argued that this is in fact not random data, as the same numbers will be generated every time. However, this test reflects that the actual numbers play no role in the behavior being tested. On the other hand, the code is an example of a test case that should be divided in two. The actual code can be checked https://github.com/apache/commons-math/blob/eb57d6d457002a0bb5336d789a3381a24599affe/src/test/java/org/apache/commons/math4/linear/BlockFieldMatrixTest.java#L182[here].

[[random-data, Listing {counter:listing}]]
.Listing {listing}. Right use of random data. The test case fixes the seed, however it could be argued that it is in fact not exactly random.
[source, java]
----
// In org.apache.commons.math3.linear.BlockFieldMatrixTest

/** test copy functions */
@Test
public void testCopyFunctions() {
  Random r = new Random(66636328996002l); //<1>
  BlockFieldMatrix<Fraction> m1 = createRandomMatrix(r, 47, 83);
  BlockFieldMatrix<Fraction> m2 = 
	new BlockFieldMatrix<Fraction>(m1.getData());
  Assert.assertEquals(m1, m2);
  BlockFieldMatrix<Fraction> m3 = 	
	new BlockFieldMatrix<Fraction>(testData);
  BlockFieldMatrix<Fraction> m4 = 
	new BlockFieldMatrix<Fraction>(m3.getData());
  Assert.assertEquals(m3, m4);
}
----
<1> Using a fixed seed to ensure repeatability.

<<distribution-test>> shows another test case using random data. In this code developers are testing a random generator which should produce a collection of vectors uniformly distributed around the unit sphere. Again, developers used a fixed seed. This test case also exemplifies the use of a good strong oracle that validates the property of the distribution without assumptions on the actual numbers. Changing the seed should not not change the outcome of the test or make the test fail unnecessarily. On the other hand, the oracle might be better placed in a separate method in the form of a custom assertion, as we will explain later. The original code can be checked https://github.com/venkateshamurthy/java-quantiles/blob/master/src/test/java/org/apache/commons/math3/random/UnitSphereRandomVectorGeneratorTest.java#L29[here].

[[distribution-test, Listing {counter:listing}]]
.Listing {listing}. Another example on the use of random data. This time, he test case also has a strong verii
[source, java]
----
// In org.apache.commons.math3.random.UnitSphereRandomVectorGeneratorTest
@Test
public void test2DDistribution() {
    
    RandomGenerator rg = new JDKRandomGenerator();
    rg.setSeed(17399225432l); //<1>
    UnitSphereRandomVectorGenerator generator = new UnitSphereRandomVectorGenerator(2, rg);

    // In 2D, angles with a given vector should be uniformly distributed
    int[] angleBuckets = new int[100];
    int steps = 1000000;
    for (int i = 0; i < steps; ++i) {
        final double[] v = generator.nextVector();
        Assert.assertEquals(2, v.length);
        Assert.assertEquals(1, length(v), 1e-10);
        // Compute angle formed with vector (1,0)
        // Cosine of angle is their dot product, because both are unit length
        // Dot product here is just the first element of the vector by construction
        final double angle = FastMath.acos(v[0]);
        final int bucket = (int) (angleBuckets.length * (angle / FastMath.PI));
        ++angleBuckets[bucket];
    }
    // Simplistic test for roughly even distribution
    final int expectedBucketSize = steps / angleBuckets.length;
    for (int bucket : angleBuckets) { //<2>
        Assert.assertTrue("Bucket count " + bucket + " vs expected " + expectedBucketSize,
                            FastMath.abs(expectedBucketSize - bucket) < 350);
    }
}
----
<1> Fixed seed
<2> Strong verification


<<strong-data>> is an example of a test case with extensive data that has been carefully crafted to meet the requirements. The input data has been generated beforehand, possibly to ensure efficiency and repeatability. The generation process has been also carefully documented. The full test case can be seen https://github.com/venkateshamurthy/java-quantiles/blob/1dd682e8a00af5968ec4057b0613dd73d5eb704f/src/test/java/org/apache/commons/math3/special/GammaTest.java#L170[here].

[[strong-data, Listing {counter:listing}]]
.Listing {listing}. Example of carefully crafted input.
[source, java]
----
//In org.apache.commons.math3.special.GammaTest

    /**
     * Reference data for the {@link Gamma#logGamma(double)} function. This data
     * was generated with the following <a
     * href="http://maxima.sourceforge.net/">Maxima</a> script.
     * <pre>
     * kill(all);
     * fpprec : 64;
     * gamln(x) := log(gamma(x));
     * x : append(makelist(bfloat(i / 8), i, 1, 80),
     *     [0.8b0, 1b2, 1b3, 1b4, 1b5, 1b6, 1b7, 1b8, 1b9, 1b10]);
     * for i : 1 while i <= length(x) do
     *     print("{", float(x[i]), ",", float(gamln(x[i])), "},");
     * </pre>
     */
    private static final double[][] LOG_GAMMA_REF = {
        { 0.125 , 2.019418357553796 },
        { 0.25 , 1.288022524698077 },
        { 0.375 , 8630739822706475 },  
        //...129 more lines
    };
----

<<custom-assertion>> shows an example of a custom assertion, built to support the testing process. This is a verification used in several test cases inside the test suite. So, it is a good practice to refactor the assertion condition into a method. This is also a way to avoid *Equality Pollution*. In this case, even the JUnit style have been respected. Also notice how `doubles` are compared using a precision. Floating point types should never be compared with direct equality due to numerical errors. The code can be checked https://github.com/joulupunikki/math/blob/master/src/test/java/org/apache/commons/math3/TestUtils.java#L165[here].

[[custom-assertion, Listing {counter:listing}]]
.Listing {listing}. A custom assertion.
[source, java]
----
// In org.apache.commons.math3.TestUtils

    /**
     * Verifies that the relative error in actual vs. expected is less than or
     * equal to relativeError.  If expected is infinite or NaN, actual must be
     * the same (NaN or infinity of the same sign).
     *
     * @param msg  message to return with failure
     * @param expected expected value
     * @param actual  observed value
     * @param relativeError  maximum allowable relative error
     */
    public static void assertRelativelyEquals(String msg, double expected,
            double actual, double relativeError) {
        if (Double.isNaN(expected)) {
            Assert.assertTrue(msg, Double.isNaN(actual));
        } else if (Double.isNaN(actual)) {
            Assert.assertTrue(msg, Double.isNaN(expected));
        } else if (Double.isInfinite(actual) || Double.isInfinite(expected)) {
            Assert.assertEquals(expected, actual, relativeError);
        } else if (expected == 0.0) {
            Assert.assertEquals(msg, actual, expected, relativeError);
        } else {
            double absError = FastMath.abs(expected) * relativeError;
            Assert.assertEquals(msg, expected, actual, absError);
        }
    }

----

=== Test design

Any testing process, automatic or manual, could be abstracted as <<testing-process>> shows. The system or program under test is executed using selected test inputs. The result of the execution is evaluated with the help of an oracle based on the specification. If the oracle deems the result incorrect, then we must find the fault. Otherwise, we continue the testing process until a stopping criterion is met. 

[graphviz, testing-process, png]
.Testing process at a glance. 
....
digraph {

    input[shape="rectangle", label="Test input"];
    program[shape="rectangle", label="Program under test"];
    execution[shape="polygon", sides="6", label="Execution"];
    result[shape="rectangle", label="Result"];
    specification[shape="rectangle", label="Specification"];
    oracle[shape="polygon", sides="6", label="Oracle"];
    verdict[shape="diamond", label="Verdict"];
    stop[shape="rectangle", label="Stopping Criteria"];
    fix[shape="plaintext", label="Locate and fix the fault"];

    input -> execution;
    program -> execution;
    execution -> result;
    result -> oracle;
    specification -> oracle;
    oracle -> verdict;
    { rankdir=LR; verdict -> fix; }
    verdict -> stop;
    stop -> input[label="Not met"];
}
....

This diagram puts in evidence three main problems to be solved when designing our tests. First we need to identify a set of test inputs that will become the starting point for the tests cases. Then, for each test case we need to define a strong oracle able to tell when the result of the execution meets the requirements or not. Also we need to know how much should we test, that is, to set a stopping criterion. Ideally we would test until there are no more faults or when we explore all possible inputs, but this can not be done in practice. Locating and fixing an identified fault is also a very important problem, but it is out of the scope of testing and more related to _debugging_. However, tests failures should provide enough information for developers to find and correct the fault.

Solving these problems is far from easy. But, no matter the solution or strategy we pick, out ultimate goal should be to design tests capable of discovering faults.

==== Reachability, Infection, Propagation, Revealability

The main goal of testing is to reveal the presence of faults. However, there are four concrete conditions that a test case execution should meet to be able to discover a fault. These conditions are expressed in the _Reachability, Infection, Propagation, Revealability_ (RIPR) model <<li2016test>>.

<<ripr-faulty-program>> shows the `findLast` method. This method should return the index of the last occurrence of a given `element` in a given `array`. If the element is not present the method should return -1 and if the array is `null` it should throw an instance of `NullPointerException`. The method in question has a fault, the loop condition should be `i >= 0` instead of `i > 0`. Due to this fault this `findLast` will never compare `element` to the value in the first position of the array. 


[[ripr-faulty-program, Listing {counter:listing}]]
.Listing {listing}. `findLast` is supposed to return the index of the last occurrence of a given element in a given array. If the array is `null` the method should throw a `NullPointerException` exception. If the element is not present, then it should return -1. This method contains a fault as it never inspects the first element of the array.
[source, java]
----
public static int findLast(int[] array, int element) {
    for (int i = array.length - 1; i > 0 ; i--) { //<1>
        if(array[i] == element)
            return i;

    }
    return -1;
}
----
<1> Loop condition should be `i >= 0`. 

<<ripr-no-reachability>>, <<ripr-no-infection>>, <<ripr-no-propagation>> and <<ripr-no-revealability>> show test cases exercising `findLast`. However, all these test cases fail to observe the bug for different reasons. We shall use these test cases to illustrate the concepts behind the RIPR model. We shall also discover how the model can explain why the fault is not discovered.

Reachability::

For a test case to discover a fault it must first execute/_reach_, the code location where the bug is present. The test case in <<ripr-no-reachability>> tests the behavior of the method when the input array is `null`. Therefore this test case never executes the loop condition and does not reach the fault.
+
[[ripr-no-reachability, Listing {counter:listing}]]
.Listing {listing}. A test case that does not reach the fault.
[source, java]
----
@Test
public void testNull() {
    assertThrows(NullPointerException.class, ()  -> {
        findLast(null, 1);
    });
}
----

Infection::

Reaching the location of the fault is not the only condition the execution should meet to discover the fault. The test case should also produce an incorrect program state, that is, it should _infect_ the program state with incorrect values. <<ripr-no-infection>> fails to do that. In this test case the element in the first position of the array is never inspected. The last occurrence of the element given as input is found at the last position of the array. The test case has the same behavior in the presence of the fault as is the program was correct. On the other hand, <<ripr-no-propagation>> do infect the state of the program. The first position is not checked in the faulty version which is not the case for the correct program. So, the infection comes for the fact `i` is never `0` during the execution while it should have had this value at some point.
+
[[ripr-no-infection, Listing {counter:listing}]]
.Listing {listing}. A test case that reaches the location of the fault but does not infect the program state.
[source, java]
----
@Test
public void testLastElement() {
    int[] array = {0, 1, 2};
    assertEquals(array.length - 1, findLast(array, 2));
}
----

Propagation::

<<ripr-no-propagation>> do infect the program but does not reveal the fault. A test case must reach the location of the fault, infect the program state but also _propagate_ the program state infection to the rest of the execution. <<<ripr-no-propagation>> produces an incorrect program state as the first position of the array is not inspected but returns the right result, so the error does not even reach the code of the test case. On the other hand, <<ripr-no-revealability>> do propagate the infection to an observable code location. In this new test case `findLast` produces a wrong result. However, the test case is not yet able to detect the fault.
+
[[ripr-no-propagation, Listing {counter:listing}]]
.Listing {listing}. A test case that reaches the fault, infects the program state but does not propagate the infection.
[source, java]
----
@Test
public void testNotFound() {
    assertEquals(-1, findLast(new int[]{0, 1, 2}, 4));
}
----

Revealability::

It is impractical, if not impossible, to create an oracle that observes the entire program state. That is why, for a test case to discover a fault, it must not only reach the location, infect the program state and propagate the infection to the rest of the execution. The test must also observe the right portion of the state and use a strong condition to verify it. <<ripr-no-revealability>> reaches the fault, infects the program state, produces a wrong result that propagates to the code of the test case, but the oracle is not adequate. The condition of the result to be greater than zero is met by the incorrect result `2`, it is an example of a weak oracle and the test case fails to _reveal_ the fault. 
+
[[ripr-no-revealability, Listing {counter:listing}]]
.Listing {listing}. A test case that fails to reveal the fault, due to a weak oracle. The method returns a worng value `2` when the correct value is `0`. Both values meet the assertion.
[source, java]
----
@Test
public void testRepeated() {
    assertTrue(findLast( new int[]{0, 1, 0}, 0) >= 0);
}
----
+
Only the test case in <<ripr-test>> meets all the conditions to reveal the fault in the method. The method produces an incorrect value, `-1`, which is not the expected value `0`. When this new test case is executed all conditions are met. The fault is reached, the state is infected, the infection is propagated to the code of the test case, there is an oracle observing the right portion of the program state and the oracle is strong enough to make the test case fail with the wrong result.
+
[[ripr-test, Listing {counter:listing}]]
.Listing {listing}. A test case able to reveal the fault.
[source, java]
----
@Test
public void testFirst() {
    assertEquals(0, findLast(new int[]{0, 1, 2}, 0));
}
----

Not all test cases should discover all faults. Test cases should remain simple as we discussed before. A single test case can not cover all possible execution scenarios and can not discover all potential faults. That is why a test suite should be conformed by many different test cases. These test cases should be carefully designed to select the right input able to reach faults, infect the program state and propagate the infection. Then, test cases need strong oracles to discover the fault. The design of a test suite should be guided by concrete criteria ensuring that these conditions if there is a fault. These criteria not only will ensure the creation of good test cases, they will also provide a concrete way to evaluate the quality of a test suite as a whole.

==== Coverage criteria for test qualification

Designing tests is hard. We need to choose good inputs to ensure potential faults are _reached_ and that their effects do propagate to an observable point in the program execution. We also need to design strong oracles so the faults can be discovered and we need to know how many test cases our test suite should have to assure certain level of quality in the testing process.Formal *Coverage criteria* help testers solve these problems.

According to Ammann and Offutt <<ammann2016introduction>>, a  _coverage criterion_ can be seen as a collection of rules that impose _test requirements_ for a test suite. A _test requirement_ is a specific element of a software artifact that a test case must satisfy or cover.

Perhaps the most widely used coverage criterion nowadays in industry is _statement coverage_. This coverage establishes each program statement as a test requirement, that is, it expects the test suite to execute all statements in the code of the program under test. So, following this criterion we create test cases until all statements are executed by the test suite.

In practice it is quite hard, sometimes not even desirable, that all test requirements of a coverage criterion are satisfied or covered by the test suite. For example, making test cases just to execute statements from simple getter methods might be a waste of resources. 

A coverage criterion has a _coverage level_ associated. The coverage level is the ratio of test requirements that are covered by the test suite. For statement coverage this is the percentage of statements in the program that are executed by the tests.

Coverage criteria help testers create more effective and efficient test suites, with fewer tests cases and better fault detection capabilities. Following coverage criteria we are able to better explore the input space. A coverage criterion ensures the traceability from each test case to the test requirements they cover. The purpose of a test case becomes, clear, as it  is designed to cover a specific requirement or a specific set of requirements. Coverage criteria also set a well defined stopping condition for the test creation process and provide an effective way to evaluate the quality of the test suite.

The following sections introduce and explain some of the most relevant and known coverage criteria.

===== Input space partitioning

The _input domain_ of a system under test is the set of all possible values that the input parameters can take. If there are more than one parameter, then the input domain is the cartesian product of the domains of all parameters. The input domain also includes values that could be incorrect for the program. These are also very important for testing purposes. A test input is then a tuple of values from the domain, one for each parameter.

For example, <<isp-findlast>> shows the signature of the `findLast` method introduced first in <<ripr-no-infection>>. This method takes as input an array of integers and an integer. Therefore its input domain is a tuple of all possible integer arrays, including `null` and empty arrays and all possible integers. Test inputs for this method could be `{ array: null, element: 1 }`, `{ array: {}, element: 2 }`, `{ array: {1, 2, 3}, element: 4 }`.

[[isp-findlast, Listing {counter:listing}]]
.Listing {listing}. Method from <<ripr-no-infection>>. The input domain is the tuple of all possible arrays _i.e._ including a `null` array, an empty array and so on, and all possible integers.
[source, java]
----
public static int findLast(int[] array, int element) { ... }
----

<<isp-isvaliddate>> shows the signature of a method that takes three integers and says if they form a valid date according to the https://en.wikipedia.org/wiki/Gregorian_calendar[Gregorian Calendar]. The input domain is the set of all possible tuples of three integer elements, including negative integers and zero. Possible test inputs may include: `{ day:  1, month:  2, year:  200 }`, `{ day: 19, month:  9, year: 1983 }` or `{ day: 33, month: 12, year: 1990 }`.

[[isp-isvaliddate, Listing {counter:listing}]]
.Listing {listing}. A method to check is three integers form a valid date. The input domain is the set of all possible tuples of integers, including negative integers and zero. 
[source, java]
----
public static boolean isValidDate(int day, int month, int year) { ... }
----

<<isp-stack>> shows an extract of the `BoundedStack` class shown in <<stack-example>>. In case we are testing methods `push` and `pop`, we should consider all possible values of `elements` and `count`. That is, when testing classes, instance fields, and even global static fields used by the method are also part of the input. Observe that in this case, `elements` will never be `null` since it is created in the constructor.

[[isp-stack, Listing {counter:listing}]]
.Listing {listing}. Extract from the `BoundedStack` class presented in <<stack-example>>. Observe that all values of the fields `elements` and `count` form part of the input domain for the `push` and `pop` methods.
[source, java]
----

class BoundedStack {
    private int[] elements;
    private int count;

    ...

    public void push(int item) {
        if(count == elements.length) {
            throw new IllegalOperationException();
        }
        count = count + 1;
        elements[count] = item;
    }

    public int pop(int item) {
        if(elements == 0) {
            throw new IllegalOperationException();
        }
        count = count - 1;
        return elements[count];
    }

    ...
}

----

The _input space partitioning_ technique design tests based on a model of the input domain. From this model it is possible to derive several coverage criteria which result in a broad selection of potential test inputs. The technique uses only the interface of the program, the signature of the method or even the specification, but does not need to observe the internal structure of the artifact being tested. In this sense it is said to be a `blackbox` technique, as opposed to `whitebox` techniques, that heavily rely on the internal structure, for example, the code of the method.

To model the input, this technique creates partitions of the domain. A partition is a subdivision of the domain into subsets or _blocks_ in such a way that the union of all blocks results in the entire input domain and all blocks are disjoint, that is, no element, or test input, can be included in more than one block for the same partition. Each partition is created by identifying characteristics which describe the structure of the input domain. Characteristics and blocks should be designed in such a way that all values in one block are equivalent according to the characteristic that defines the partition.

Identifying the right characteristics is hard and requires expertise. There are two main approaches: _interface based modeling_ and _functionality based modeling_.

Interface based modeling considers each parameter separately and takes information only from their specific domain. It is a simple alternative that makes it easier to identify the characteristics. However, it does not use all the information available like the specification and fails to capture the interaction between parameters.

Using interface based modeling to describe the input of `findLast` from <<isp-findlast>> we may identify the characteristics shown in <<isp-interface-findlast>>. Two characteristics are identified: _`array` is `null`_ and _`array` is empty_. Each characteristic defines a partition with two blocks, one to contain the arrays for which the condition of the characteristic is true and another containing arrays for which the characteristic is false.

[[isp-interface-findlast]]
.Characteristics and blocks identified for `findLast` from <<isp-findlast>> considering only the `array` parameter.
[options="header"]
|===
| Characteristics    2+| Blocks
| `array` is `null`    | _True_ | _False_
| `array` is empty     | _True_ | _False_
|===

The same could be done in the parameter `element`, but it will not yield characteristics interesting enough for the tests. The values of `element` are irrelevant in isolation. It makes sense to look at them only in relation to the content of `array`.

Functionality based modeling uses semantic information and plays with the specification, the domain of each parameter and the interplay between the values of different parameters. Identifying good characteristics with this approach is harder but may yield better results.

For the same `findLast` method in <<isp-findlast>>, with this approach we may identify the characteristics in <<isp-functionality-findlast>>. The table shows a characteristic that captures the number of times `element` appears in `array` which yields three blocks: one for arrays that do not contain `element`, one for arrays containing only one occurrence of `element`, and another for arrays in which `element` appears more than once. The other two characteristics consider the position of `element` in the `array` and each of them yields a partition with two block.

[[isp-functionality-findlast]]
.Characteristics identified using functionality based modeling for `findLast` from <<isp-findlast>>.
|===
| Characteristics 3+| Blocks
| Number of times `element` appears in `array` | 0      | 1       | > 1 
| `element` appears in the first position      | _True_ | _False_ |
| `element` appears in the last position       | _True_ | _False_ |
|===

All characteristics in <<isp-interface-findlast>> and <<isp-functionality-findlast>> could be used in conjunction to design test inputs.

Now we shall model the input of `isValidDate` from <<isp-isvaliddate>>. We first identify characteristics with the interface based approach. This may yield the following result:

|===
| Characteristics  2+| Blocks 
| Value of `year`    | \<= 0 | > 0
| Value of `month`   | \<= 0 | > 0
| Value of `day`     | \<= 0 | > 0
|===

There is one characteristic for each parameter and they consider their values separately with respect to their domain. All possible values, valid or invalid are included and all blocks for the same characteristic are disjoint.
Values close to the boundaries between valid and invalid inputs tend to be problematic and often source of bugs. So it is a good idea to include blocks reflecting these values. This way we can expand our initial characteristics as follows:

|===
| Characteristics  3+| Blocks 
| Value of `year`    | < 0 | 0 | > 0
| Value of `month`   | < 0 | 0 | > 0
| Value of `day`     | < 0 | 0 | > 0
|===

These blocks may be too broad for testing purposes. Sometimes it is useful to partition blocks into sub-partitions specially in the case of valid inputs. 

In our example, the meaningful values of `month` and `day` depend on each other and the value of `year`. Actually, the number of valid days depend on the month, and even the year in the case of February, so we turn to a functionality based approach for new characteristics. 

We first include the notion of leap year and subdivide years greater than 0 into leap and non-leap. Another almost equivalent solution for this could be to add a new characteristic reflecting this condition. We then include a block for valid month numbers and another for valid days which depend on the maximum valid number according to the month, represented as `max(month, year)`.

|===
| Characteristics  4+| Blocks 
| Value of `year`    | < 0 | 0 | valid leap year                   | valid non leap year
| Value of `month`   | < 0 | 0 | >= 1 and \<= 12]                  | > 12
| Value of `day`     | < 0 | 0 | >= 1 and \<= max(`month`, `year`) | > max(`month`, `year`)` 
|===

We can go further and sub-partition valid month numbers into groups matching the maximum number of days on each. The result would be as follows:

[[final-partitions]]
.Final partitions and blocks for `isValidDate`. Each partition and block have been named for future reference.
|===
2.2+| Characteristics     6+| Blocks 
                            h| b1  h| b2 h| b3                                h| b4                          h| b5 h| b6
h| q1 | Value of `year`      | < 0  | 0   | valid leap year                    | valid common year            |     |
h| q2 | Value of `month`     | < 0  | 0   | { 1, 3, 5, 7, 8, 10, 12}           | { 4, 6, 9, 11 }              | 2   | > 12
h| q3 | Value of `day`       | < 0  | 0   | >= 1 and \<= max(`month`, `year`)  | > max(`month`, `year`)       |     |
|===

Now we have a block for months with 31 days, another for months with 30 days and one for February which is a very special case.

NOTE: Notice that this is not the only input model that we can design, and it might not even be the optimal. For example, it could be argued that, for this particular method, the blocks where each parameter is zero is equivalent to the blocks where each parameter is negative. _There is no silver bullet_ when it comes to modeling the input. That is why experience and knowledge about the application domain are so important here.

If the input contains a parameter with a enumerative domain of a few values, it could make sense to create a partition with one block per value. In our example we could have one block for each month, but there is no substantial different among months with the same amount of days for this particular method we are testing.

It should be taken into account that, when testing classes, different methods of the same class could share the same characteristics to define partitions. For example, both the `push` and `pop` methods in the stack implementation shown in <<stack-example>> could partition the input considering when the stack is empty or not. Therefore it is a good idea, when testing a class, to first find the characteristics for all methods and reuse them.

Once the features and blocks have been identified, the concrete test inputs are built by picking values matching a selection of blocks from different partitions. For example: `{day: 1, month: 2, year:2000}` is an input matching the third block for each of the partitions identified in <<final-partitions>>, that is, blocks `q1b3`, `q2b3`and `q3b3`. 

Test inputs can be immediately translated to concrete test cases. For example, if we select blocks `q1b4`, `q2b5` and `q3b4` we can pick `{ day: 29, month: 2, year: 2019}`. The test case could be written as follows:

[source, java]
----
@Test
public void test29DaysFebruaryCommonYear () {
    assertFalse(isValidYear(29, 2, 2019), "February in common years should not have more than 28 days.");
}
----

Notice how designing test cases from block selections makes the test case clear and helps trace it back to the requirements. 

The challenge now is to create effective block combinations. For that we can use the following coverage criteria:


Each choice coverage (ECC):: This criterion sets each block from each partition as a test requirement. That is, we must select a set of inputs in such a way that all blocks are represented at least once.

The following set of inputs achieve ECC coverage. All blocks from <<final-partitions>> are covered by at least one input:

|===
| Input                               | Blocks
| `{ day:  1, month:  1, year:   -1}` | `q1b1`, `q2b3`, `q3b3` 
| `{ day: -1, month: -1, year:    0}` | `q1b2`, `q2b1`, `q3b1`
| `{ day:  0, month:  4, year: 2020}` | `q1b3`, `q2b4`, `q3b2`
| `{ day: -2, month:  0, year: 2019}` | `q1b4`, `q2b2`, `q3b1`
| `{ day: 29, month:  2, year: 2019}` | `q1b3`, `q2b5`, `q3b4`
| `{ day:  0, month: 13, year: 2018}` | `q1b4`, `q2b6`, `q3b2`
|===

Sometimes it is not feasible to select certain blocks at the same time. For example, we should not pick `q3b3`: `day` larger than the maximum according to `month` and `year`, if `month` does not have a valid value, for example if we pick `q2b1`. Such combinations can be dropped when creating test inputs. However, if an input model contains too many of these restrictions it might be a good idea to redesign the partitions.

While this coverage criterion is easy to achieve, it may not yield good results, as it may miss interesting combinations between blocks.

All combinations coverage (ACoC):: As a counterpart to ECC, to meet this criterion we must combine all blocks from all characteristics. This could lead to a very high number of combinations making it impractical. So fewer combinations are desirable. For partitions in <<final-partitions>> this criterion yields 82 test requirements or combinations after dropping unfeasible block selections.

Pair-wise coverage (PWC):: Instead of all combinations, a value from each block, for each partition must be combined with a value from every block for each other partition. That is, all pair of blocks from different partitions are selected as test requirements. For partitions in <<final-partitions>> this criterion produces 62 test requirements, which can be covered by only 25 inputs. The number of inputs could still be high for some partitions.

An extension of PWC is to combine `T` characteristics or partitions at the same time, but it has been shown that this does not produce any improvement in practice.

Combining more than one invalid value is not useful in general. Most of the times, the program recognizes only of them and the effects of the others are masked. The following two criteria provide a good alternative to avoid that and produce a smaller number of inputs.

Base choice coverage (BCC):: A _base choice_ block is chosen for each partition or characteristic. A _base test_ is formed with the base choice for each partition. Subsequent test inputs are formed by changing only one base choice from this base test and replacing it with another non-base block for the same partition and keeping the others. Base choices should always be the simplest, smallest, most common choices.

Multiple base choice coverage (MBCC):: It is an extension of the previous criterion. This criterion may select more than one base choice for each partition. An initial test set is built using an ECC coverage on the base choices, then subsequent inputs are formed in the same way as in BCC: by replacing one base choice for another non-base block in the same partition and keeping the others.

For partitions in <<final-partitions>>, we can pick `q1b3` and `q1b4` as base choices for the first characteristic, `q2b3`, `q2b4`, `q2b5` for the second and `q3b3` for the third one. With this setup we could pick the following inputs covering base choices: `{ day:  1, month: 3, year: 2019 }`, `{ day: 30, month: 9, year: 2018 }`, `{ day: 29, month: 2, year: 2020 }`. Then, more inputs could be added by exchanging one basic block by another non-basic choice. For example, if we take `{ day: 29, month: 2, year: 2020 }`, it matches blocks `q1b4`, `q2b5` and `q3b3`. Changing `q3b3` by `q3b4` implies to change the value of `day` to a value larger than the maximum according to the month. With this we could change 29 by 30 and obtain a new input `{ day: 30, month: 2, year: 2020 }`. The process continues until no new inputs can be added.


Input space partitioning helps defining an initial set of tests inputs. However, the criteria explained in this section do not ensure any of the conditions to discover a fault as stated by the RIPR model.

===== Structural coverage

As their name implies, _structural coverage criteria_ rely on the internal structure of the artifact under test, that is, the code of the method, class or complete program we are testing. This section present some of the more commonly used criteria in this category.

Statement coverage::
The simplest structural criterion is *statement coverage*. This criterion establishes each statement in a program as a test requirement, that is, the test suite should be designed in such a way that all statements in the program are executed by at least one test case. In this way it ensures the _reachability_ of the RIPR model. If all statements are covered, then all potential faults will be reached.
+
It is a simple criterion, easy to interpret and also very easy and fast to compute. Nowadays, most practitioners use the _statement coverage level_, that is, the ratio of statements executed by the test suite as a proxy for the quality of their tests. The practice is so common that the statement coverage level is known as _code coverage_ or simply _coverage_. There are many available tools to compute code coverage and they are supported by most mainstream IDEs and CI/CD servers.
+
<<findlast-coverage-found>> highlights the statements executed/covered/reached by the test case shown in <<ripr-no-infection>> on the code of the method included in <<ripr-faulty-program>>. With coverage information it is easy to see that, in the absence of more test cases, we miss a test where the element could not be found in the array, as the last statement is not executed.
+
[[findlast-coverage-found, Listing {counter:listing}]]
.Listing {listing}. Statements covered by the test case in <<ripr-no-infection>> on the code of the method from <<ripr-faulty-program>>.
[source,java,linenums,highlight=2..4]
----
public static int findLast(int[] array, int element) {
    for (int i = array.length - 1; i > 0 ; i--) {
        if(array[i] == element)
            return i;

    }
    return -1;
}
----
+
The last statement can be covered by a test case such as the one shown in <<ripr-no-propagation>>. <<findlast-coverage-not-found>> highlights the statements covered by this test case on the code from <<ripr-faulty-program>>.
+
[[findlast-coverage-not-found, Listing {counter:listing}]]
.Listing {listing}.
[source,java,linenums,highlight="2,3,7"]
----
public static int findLast(int[] array, int element) {
    for (int i = array.length - 1; i > 0 ; i--) {
        if(array[i] == element)
            return i;

    }
    return -1;
}
----
+
Notice that both test cases together cover all the instructions of the method. However they are not able to discover the fault. This criterion ensures reachability but does not ensure any of the other conditions for the fault to be found. It is extremely useful to rapidly known which pieces of code haven't been tested but is not a good quality metric for a test suite.

Other structural criteria can be defined over the control flow graph of a method or a sequence of instructions. Recalling the procedure explained in <<cyclomatic>>, the control flow graph of a method is built as follows:

* Initially, the graph has two special nodes: the _start_ node and the _end_ node.
* A sequence of instructions with no branches is called a _basic block_. Each basic block becomes a node of the graph.
* Each branch in the code becomes an edge. The direction of edge coincides with the direction of the branch.
* There is an edge from the start node to the node with the first instruction.
* There is an edge from all nodes that could terminate the execution of the code, to the end node.

<<control-flow-findlast>> shows the control flow of the `findLast` method presented in <<ripr-faulty-program>>.

[[control-flow-findlast]]
[graphviz, control-flow-findlast, png]
.Control flow graph from the `findLast` method, presented in <<ripr-faulty-program>>. 
....
digraph {

    node[shape=rectangle];
    start[shape=ellipse, group=main];
    init_i[label="(1): int i = array.length - 1", group=main];
    is_element[label="(3): array[i] == element", group=main, shape="diamond"];
    {
        rank = same;
        greater_than_zero[label="(2): i > 0", group=main, shape="diamond"];
        decrement_i[label="(4): i--"];
    }
    {
        rank = same;
        return_i[label="(5): return i", group=main];
        return_minus_one[label="(6): return -1"];
    }
    end[shape=ellipse, group=main];
    
    start -> init_i;
    init_i -> greater_than_zero;
    greater_than_zero -> is_element[label = true];
    greater_than_zero -> return_minus_one[label = false];
    return_minus_one -> end;
    is_element -> decrement_i[label = false];
    decrement_i -> greater_than_zero;
    is_element -> return_i[label = true];
    return_i -> end;
}
....

The execution of a test case produces an _execution path_ or _execution test_ over the control flow graph. This path goes from the `start` node to the `end` node and includes all nodes containing instructions executed by the test case and all edges connecting those nodes. 

For example, the test case in <<ripr-no-infection>> produces the following path `start => (1) => (2) => (3) => (5) => end`. With this path, the test case covers the nodes/blocks `(1)`, `(2)`, `(3)` and `(5)`. Nodes `start` and `end` are covered by all execution paths. The same path covers edges `start => (1)`, `(1) => (2)`, `(2) => (3)`, `(3) => (5)`, `(5) => end`. The test does not cover nodes `(4)` and `(6)` and edges `(3) => (4)`, `(4) => (2)`, `(2) => (6)` and `(6) => end`. <<coverage-over-cfg>> shows in blue the elements covered by this test case in the graph.

[[coverage-over-cfg]]
[graphviz, coverage-over-cfg, png]
.In blue we show the elements of the control flow graph from <<ripr-faulty-program>> covered by the execution of the tests case shown in <<ripr-no-infection>>. 
....
digraph {

    node[shape=rectangle, color=darkslateblue, fontcolor=white style=filled];

    start[shape=ellipse, group=main]; 
    init_i[label="(1): int i = array.length - 1", group=main];
    is_element[label="(3): array[i] == element", group=main, shape="diamond"];
    {
        rank = same;
        greater_than_zero[label="(2): i > 0", group=main, shape="diamond"];
        decrement_i[label="(4): i--", color=black, fillcolor=white, fontcolor=black];
    }
    {
        rank = same;
        return_i[label="(5): return i", group=main];
        return_minus_one[label="(6): return -1", color=black, fillcolor=white, fontcolor=black];
    }
    end[shape=ellipse, group=main];
    
    start -> init_i[color=darkslateblue,];
    init_i -> greater_than_zero[color=darkslateblue];
    greater_than_zero -> is_element[label = true, fontcolor=darkslateblue, color=darkslateblue];
    greater_than_zero -> return_minus_one[label = false];
    return_minus_one -> end;
    is_element -> decrement_i[label = false];
    decrement_i -> greater_than_zero;
    is_element -> return_i[label = true, fontcolor=darkslateblue, color=darkslateblue];
    return_i -> end[color=darkslateblue];
}
....

The following coverage criteria are precisely defined over the nodes, edges and paths of a control flow graph.

Block coverage::
With *block coverage* we consider each basic block or control flow graph node as a test requirement. This is very related to statement coverage, as basic blocks guarantee that if one instruction from the block is executed, all the other instructions in the same block will be executed as well. In fact, some tools actually compute block coverage to report statement coverage. In the example discussed before, the test from <<ripr-no-propagation>> was able to cover all blocks but nodes `(4)` and `(6)`.

Branch coverage::
This criterion sets branches in the program, that is, edges in the control flow graph as test requirements. Instead of the nodes, here we consider the edges in the execution paths. It helps determine whether all outcomes from decision branches have been explored by the test cases. In the example discussed before, the test from <<ripr-no-propagation>> was able to cover all edges but `(3) => (4)`, `(4) => (2)`, `(2) => (6)` and `(6) => end`. 

Path coverage::
This criterion sets all possible execution paths as test requirements. That is, we aim to design a test suite that traces all execution paths in the control flow graph. Executing all possible paths leads to exhaustive testing, and this is, however ideal, not possible in practice. For example, if the control flow graph contains a loop as in <<control-flow-findlast>> the number of possible execution paths is infinite. Therefore, we need to select which paths to cover in practice. The two following criteria are examples of how to select which paths to execute. 

Path basis testing::
A directed graph is said to be _strongly connected_ if for every pair of nodes we can find a path that starts in the first node and ends in the second node. A control flow graph can be made strongly connected if we add a bogus edge from the end node to the star node. A circuit in a graph is which ends at the same node it begins.
+
If we add the bogus edge, the execution of the test case in <<ripr-no-infection>> produces the circuit `start => (1) => (2) => (3) => (5) => end => start`.
+
A set of circuits from a graph is said to be linearly independent if all of the circuits differ in at least one edge. This set of circuits is said to be a basis for all circuits in the graph if all edges are included in at least one circuit from the set. All circuits in the graph can be formed by combining circuits in the base.
+
If a directed graph is strongly connected, then, the cyclomatic complexity, as discussed in <<cyclomatic-section>>, is equal to the maximum number of linearly independent circuits, that is, the number of circuits in the base. The basis is not unique. In general we can find more than one basis for the same graph. 
+
For the graph in <<control-flow-findlast>> the number of circuits in the basis is 3 which is the number of conditionals: 2 plus 1. The following could be a basis of circuits:
+
. `start => (1) => (2) => (3) => (5) => end => start`
. `start => (1) => (2) => (6) => end => start`
. `(2) => (3) => (4) => (2)`
 
+
If we set all linearly independent circuits as test requirements, then we create a test for each circuit in the base and we ensure that we are testing, at least once, each outcome from a decision or conditional node. For example, the test case in <<ripr-no-infection>> covers the first circuit in the basis, in which the element appears in the last position. Testing with an empty array would cover the second circuit in the basis. Any test case that loops over the array covers the third and last circuit in the basis, such as the test case in <<ripr-no-propagation>>. See that with these three test cases we are able to cover the entire basis but we still do not reveal the fault.

The cyclomatic complexity sets a lower bound for the number of different tests executing all branches. In this example the cyclomatic complexity is 3 which is also the number of test cases that cover the basis.

Prime path coverage::
A _simple path_ in a graph is a path where no node appears more than once, except perhaps the first node, that could be also the last one. A _prime path_ in a graph is a simple path that is no proper sub-path of any other simple path. This means that a prime path is a simple maximal path.
+
In <<control-flow-findlast>> `(2) => (3) => (4) => (2)` is a prime path that starts and ends in `(2)`. Computing all prime paths from a graph is simple. We start with all edges, which are simple paths of length one. From there we form more simple paths by adding edges until we reach the initial node, there are no more edges to add or the next node already appears somewhere in the path. Finally, we keep the paths that are no proper sub-paths of any other. For the graph of the example the prime paths are:

. `start => (1) => (2) => (3) => (5) => end`
. `start => (1) => (2) => (6) => end`
. `start => (1) => (2) => (3) => (4)`
. `(4) => (2) => (3) => (5) => end`
. `(3) => (4) => (2) => (6) => end`
. `(2) => (3) => (4) => (2)`
. `(4) => (2) => (3) => (4)`
. `(3) => (4) => (2) => (3)`
 
+
Prime path coverage sets each prime path as a test requirement. That is, at least one test case should cover each prime path. For example, <<ripr-no-infection>> covers the first prime path of the list. The test case in <<ripr-no-propagation>> produces the following path: `start => (1) => (2) => (3) => (4) => (2) => (3) => (4) => (2) => (3) => (4) => (2) => (6) => end`. This test traces the entire array and does not fin the element. It is able to cover prime paths 2, 5, 6, 7 and 8 from the list above. This criterion is useful when testing loops in a program. It ensures the creation of tests cases that skip the loops and test cases that execute several iterations of each loop. In our example, prime path 2 corresponds to a scenario where the `for` loop is never entered, _i.e._ `array` is empty. The execution of prime paths 6, 7, 8 produces tests performing more than one iteration.

Many software artifacts can be represented as graphs. As an example, _Finite State Machines_ are a special type of graph that have been widely used to represent the behavior of software elements. Actually, most embedded software is modeled as a state machine. Finite state machines can also be used to describe specifications for software and even how a user interface should behave.

In a finite state machine, a node represent a state of the artifact and edges represent changes in those states. As an example, recall <<microwave>> where a state machine was used to model the behavior of a microwave oven. 

The structural criteria defined over the control flow graph of a method can be extended to any type of graph. Branch coverage can be generalized as _edge coverage_ and block coverage is just a special case of _node coverage_. In the particular case of finite state machines, designing test cases to achieve edge coverage ensures that all state transitions are explored and targeting node coverage ensures that all possible states of the artifact have been observed in at least one test case.

But, as explained before, the structural criteria discussed in this section only ensure that most parts of the software under test are executed. Therefore they guarantee that, if there is a fault, it will be executed by at least one test case. However, these criteria do not ensure that the input will infect the program state, or that the effects will be propagated to an observable point or that there will be an oracle strong enough observing the right portion of the state.

===== Logic coverage

A predicate is a boolean expression. They are used in all kinds of software. Predicates define branches in source code and also define the state of finite state machines like the microwave oven in <<microwave>>.

The coverage criteria presented in this section are designed specifically for predicates. They are also designed in such a way that, if there is a fault, test cases created with the help of these criteria guarantee that the state of the program will be, at least, infected by the fault. This means that, if there is a fault in the code of the predicate, there will be at least one test case where the predicate will produce the wrong result.

A predicate is defined as follows:

* A clause is a predicate. Clauses are the shortest predicates. They are the simplest boolean expressions that do not contain logical operators. They can be:
 
    ** a variable reference _i.e._ stem:[a]
    ** a comparison or any other relational expression _i.e_ stem:[a < 3], stem:[a = b]
    ** a function call _i.e._ stem:[f(a, b+3)]

* If stem:[p] and stem:[q] are predicates, then the following are also predicates:
    ** stem:[\neg p] (negation)
    ** stem:[p \wedge q] (conjunction)
    ** stem:[p \vee q] (disjunction)
    ** stem:[p \implies q] (implication)
    ** stem:[p \iff q] (equivalence)
    ** stem:[p \oplus q] (exclusive disjunction)

stem:[ a < 3 \vee c \implies fn(b)] is a predicate with three clauses: stem:[a < 3], stem:[c] and stem:[fn(b)].

<<isleapyear-predicate>> shows the `isLeapYear` method which implements a predicate to determine if a given integer represents a leap year or not. The predicate has three clauses, stem:[c_400]: `year % 400 == 0`, stem:[c_4] `year % 4 == 0` and stem:[c_!100] `year % 100 != 0`, that, for convenience could be seen as the negated form of stem:[c_100]: `year % 100 == 0`. In this example the clauses are not independent from each other. Also, given the short-circuit behavior of logical operators in most programming languages, the order in which the clauses are evaluated play a role in the evaluation of the predicate.

[[isleapyear-predicate, Listing {counter:listing}]]
.Listing {listing}. A method that says if a given integer value corresponds to a leap year or not.
[source, java]
----
public static boolean isLeapYear(int year) {
    return year % 400 == 0 || ( year % 4 == 0 & year % 100 != 0 );
}
----

The simplest logic coverage criteria sets test requirements according to the value that takes the predicate as a whole and the value of each clause.

Predicate Coverage (PC):: This coverage criterion sets two test requirements: one on which the predicate evaluates to `true` and another where the predicate evaluates to `false`.
+
To fulfill this criterion we require only two inputs one making the predicate `true` and another making the predicate `false`. It is a simple criterion but the clauses are not considered individually. Taking the predicate, stem:[((a > b) \wedge C) \vee p(x)] we can cover both requirements with stem:[(a = 5, b = 4, C = true, p(x) = true)] and stem:[(a = 5, b = 4, C = true, p(x) = false)], however, in both interpretations, the first two clauses of the predicate have always the same value.

Clause Coverage (CC):: This criterion sets two test requirements for each clause in a predicate. For one requirement, the selected clause should be `true` and for the other the same clause should be false `false`.
+
For the predicate stem:[((a > b) \wedge C) \vee p(x)] we have six requirements, two for each clause. The criterion can be covered using the following two sets of values: stem:[(a = 5, b = 4, C = true, p(x) = true)] and stem:[(a = 5, b = 6, C = false, p(x) = false)]. The first set of values covers all requirements where clauses are `true` and the second covers the scenarios where the clauses evaluate to `false`. So, with only two inputs it is possible to cover all six requirements.
+
This criterion does enforce combinations between the clauses. Also, it is possible to fulfill clause coverage and not predicate coverage at the same time. Take as example the predicate stem:[a \vee b]. Using stem:[a = true, b = false)] and stem:[(a = false, b = true)] we can cover all clauses but the predicate always evaluates to `true`.

Combinatorial Coverage (CoC):: This criterion sets a test requirement for each combination or truth value of all clauses.
+
For example, for the predicate stem:[[((a > b) \wedge C) \vee p(x)]] we create the following test requirements:
+
[options="header"]
|===
| stem:[a > b] | stem:[C] | stem:[p(x)] | stem:[((a > b) \wedge C) \vee p(x)]
| `true`       | `true`   | `true`      | `true` 
| `true`       | `true`   | `false`     | `true`
| `true`       | `false`  | `true`      | `true`
| `true`       | `false`  | `false`     | `false`
| `false`      | `true`   | `true`      | `true`
| `false`      | `true`   | `false`     | `false`
| `false`      | `false`  | `true`      | `true`
| `false`      | `false`  | `false`     | `false`
|===
+
This criterion accounts for any truth value combination of all clauses and ensures all possible values for the predicate. However, it is not feasible in practice for large predicates. The number of requirements is exponential with respect to the number of clauses in the predicate, that is  we have stem:[2^N] requirements for stem:[N] clauses.

So, we need a set of criteria able to evaluate the effects of each clause over the result of the predicate while keeping the number of test requirements reasonable low.

Active Clause Coverage:: This criterion verifies each clause under conditions where they affect the value of the entire predicate. When we select a clause, we call it a _major clause_ and rest are called _minor clauses_. So when defining the requirements for this criterion each clause become _major_ at some point.
+
A major clause _determines_ the predicate if the minor clauses have values such that changing the truth value of the major clause also changes the value of the predicate.
+
For the predicate  stem:[((a > b) \wedge C) \vee p(x)] if we select stem:[a > b] as the major clause, stem:[C] and stem:[p(x)] are the minor clauses. If we assign values stem:[(C = true, p(x) = false)] to the minor clauses, stem:[a > b] determines the predicate because, when the major clause is `true` the entire predicate is `true` and when the major clause is `false` the predicate evaluates to `false`.
+
This criterion selects each clause in the predicate as a major clause. Then, for each major clause stem:[c], we select values for the minor clauses in a way that stem:[c] determines the predicate. Then the criterion sets one test requirement for stem:[c = true] and another for stem:[c = false].
+
For example, for the predicate stem:[p = a \vee b] is easy to see that each clause determines the predicate when the other is `false`. So, selecting stem:[a] as the major clause we obtain requirements stem:[(a = true, b = false)] and stem:[(a = false, b = false)]. We proceed in a similar way with clause stem:[b] and obtain stem:[(a = false, b = true)] and stem:[(a = false, b = false)]. One of the requirements is common for both clauses, so in the end we have three test requirements stem:[(a = true, b = false)], stem:[(a = false, b = true)] and stem:[(a = false, b = false)].
+
A major challenge for the active clause criterion is the handling of the minor clauses. We need first to obtain the possible values for the minor clauses to make the major clause determine the predicate.
+
Given a predicate stem:[p] and a major clause stem:[c \in p], the predicate stem:[p_c = p_{c=true} \oplus p_{c=false}] represents the condition for stem:[c] to determine stem:[p]. That is, the values of the minor clauses that make stem:[p_c] be `true` also make stem:[c] determine stem:[p].
+
The following examples illustrate how to use this result to find the values for the minor clauses.
+
WARNING: The following examples make heavy use of https://en.wikipedia.org/wiki/Boolean_algebra[_Boolean Algebra_]. It would be better to refresh the main rules before going any further.
+
Take the predicate stem:[p = a \wedge b], selecting stem:[a] as the major clause we need to find a condition for stem:[b] so stem:[a] dominates the predicate. We proceed as follows:
+
[latexmath]
++++
\begin{eqnarray*}
    p_a & = & p_{a=true} \oplus p_{a=false} \\
        & = & (true \wedge b) \oplus (false \wedge b) \\
        & = & true \oplus b \\
        & = & \neg b
\end{eqnarray*}
++++
+
Which means that stem:[a] determines the predicate, only when stem:[b] is `false`.
+
Let's take now the predicate stem:[p = a \wedge (b \vee c)] and again stem:[a] as the major clause. We obtain: 
+
[latexmath]
++++
\begin{eqnarray*}
    p_a & = & p_{a=true} \oplus p_{a=false} \\
        & = & (true \wedge (b \vee c)) \oplus (false \wedge (b \vee c)) \\
        & = & (b \vee c) \oplus false \\
        & = & (b \vee c)
\end{eqnarray*}
++++
+
This means that any values for stem:[b] or stem:[c] making stem:[b \vee c] `true` will also make stem:[a] determine stem:[p]. In this case we could use stem:[(b = true, c = true)], stem:[(b = false, c = true)] or stem:[(b = true, c = false)].
+
On another example, if we take stem:[p = a \iff b], we obtain:
+
[latexmath]
++++
\begin{eqnarray*}
    p_a & = & p_{a=true} \oplus p_{a=false} \\
        & = & (true \iff b) \oplus (false \iff b) \\
        & = & b \oplus \neg b \\
        & = & true
\end{eqnarray*}
++++
+
We observe that stem:[p_a] is always `true`. Therefore, stem:[a] determines stem:[p] no matter the value of stem:[b]. On the other hand, if could be that stem:[p_a] would result in `false`. In that case it would be impossible for the clause stem:[a] to determine stem:[p].
+
As seen in the examples above, we can obtain four possible outcome when finding the condition for a major clause to determine the predicate: *(1)* the clause can not determine the predicate, 
*(2)* there is only one possible assignment (also known as interpretation) for the values of the minor clauses, *(3)* there is more than one possible interpretation and *(4)* the clause always determines the predicate. 

+
ACC sets two test requirements for each major clause: one in which the clause is `true` and another in which the clause is `false`. If there is more than one possible interpretation for the minor clauses we may decide to use different values on each requirement or force the same values of the minor clauses. If we force the use of the same values for the minor clauses, then we are using what is known as *Restricted Active Clause Coverage* (RACC). This is a stronger criterion but it can not be always satisfied when clauses are not independent. 

In fact, it could be possible that a test requirement becomes infeasible due to dependencies between the clauses for any of the previous coverage criteria. Given the case, we simply discard those inputs.

The literature often describes _Modified Condition/Decision Coverage_ (MC/DC). This coverage criterion is defined as follows <<comar2012formalization>>: 

[quote]
Every point of entry and exit in the program has been invoked at least once, every condition in a decision in the program has taken all possible outcomes at least once, every decision in the program has taken all possible outcomes at least once, and each condition in a decision has been shown to independently affect that decision’s outcome. A condition is shown to independently affect a decision’s outcome by varying just that condition while holding fixed all other possible conditions.

In this definition, a _decision_ is a predicate and a _condition_ is a clause. Therefore, achieving RACC for all predicates in a program ensures MC/DC.

Now, we use ACC to derive test requirements for the method in <<isleapyear-predicate>>. We rewrite the predicate encoded in the method as stem:[p = c_400 \vee (c_4 \wedge \neg c_100)], where stem:[c_x] is equivalent to `year % x == 0`.

Selecting stem:[c_400] as the major clause we obtain stem:[p_{c_400} = \neg c_4 \vee c_100]. The following table shows the values, or interpretations of the clauses that make stem:[p_{c_400}] `true`.

[options="header"]
|===
| stem:[c_4] | stem:[c_100] | stem:[p_{c_400}]
| `false`    | `true`       | `true`
| `true`     | `true`       | `true`
| `false`    | `false`      | `true`
|===

But, these clauses are not independent. In fact the first row of the table is infeasible. The same number can not be divisible by 100 (stem:[c_100 = true]) and not by 4 (stem:[c_4 = false]). Therefore we throw it away and keep only the two last rows, which leads us to the following truth values for stem:[p]:

[options="header"]
|===
| stem:[c_400] | stem:[c_4] | stem:[c_100] | stem:[p]
| `true`       | `false`    | `true`       | `true`
| `false`      | `false`    | `true`       | `false`
| `true`       | `true`     | `true`       | `true`
| `false`      | `true`     | `true`       | `false`
| `true`       | `false`    | `false`      | `true`
| `false`      | `false`    | `false`      | `false`
|===

Again, the clauses are not independent in this example. The first, second and fifth rows lead to impossible situations so we keep only the rest.

[options="header"]
|===
| stem:[c_400] | stem:[c_4] | stem:[c_100] | stem:[p]
| `true`       | `true`     | `true`       | `true`
| `false`      | `true`     | `true`       | `false`
| `false`      | `false`    | `false`      | `false`
|===

For this clause we can pick the the first and second rows to satisfy RACC.

Now we select stem:[c_4] as the major clause and obtain stem:[p_{c_4} = \neg c_100 \wedge \neg c_400] which is `true` only for the following interpretation:

[options="header"]
|===
| stem:[c_100] | stem:[c_400] | stem:[p_{c_4}]
| `false`      | `false`      | `true`
|===

Therefore, we obtain the following two additional test cases:

[options="header"]
|===
| stem:[c_400] | stem:[c_4] | stem:[c_100] | stem:[p]
| `false`      | `true`     | `false`      | `true`
| `false`      | `false`    | `false`      | `false`
|===

Finally we pick stem:[c_100] as the major clause obtaining stem:[p_{c_100} = \neg C_400 \wedge c_4]. The only possible interpretation to make stem:[c_100] dominate the predicate is:

[options="header"]
|===
| stem:[c_400] | stem:[c_4] | stem:[p_{c_100}]
| `false`      | `true`     | `true`
|===

Which leads to the following test cases:

[options="header"]
|===
| stem:[c_400] | stem:[c_4] | stem:[c_100] | stem:[p]
| `false`      | `true`     | `true`       | `false`
| `false`      | `true`     | `false`      | `true`
|===

Some of the test cases for different clauses are the same, so we combine them and obtain as final requirements the following inputs:

[options="header"]
|===
| stem:[c_400] | stem:[c_4] | stem:[c_100] | stem:[p]
| `true`       | `true`     | `true`       | `true`
| `false`      | `true`     | `true`       | `false`
| `false`      | `false`    | `false`      | `false`
| `false`      | `true`     | `false`      | `true`
|===

In the end we have created four different test cases. Ensuring ACC, also ensures PC and CC and produces fewer tests than CoC while observing the effect of each clause.

The inputs created above have to interpreted with respect to `year` which is the actual parameter of the method and defines the value of all clauses and their relationship. To satisfy the first requirement in the table above we need a value that is divisible by 4, 100 and 400 at the same time, which actually means that we need a value divisible by 400 as it implies the other two conditions. For the second requirement we need a value divisible by 100 but not by 400. The third row requires a value not divisible by 4 and the last needs one divisible by 4 but not by 100. The following table summarizes the result:

[options="header"]
|===
| stem:[c_400] | stem:[c_4] | stem:[c_100] | stem:[p] | `year`
| `true`       | `true`     | `true`       | `true`   | 2000
| `false`      | `true`     | `true`       | `false`  | 1900
| `false`      | `false`    | `false`      | `false`  | 2017
| `false`      | `true`     | `false`      | `true`   | 2020
|===

Here we picked values that resemble recent years to be closer to the intention of the method. We could have picked, 400, 100, 3 and 4 as well but these values are not so intention revealing as the others.

===== Mutation testing

We can use the coverage criteria studied so far to select meaningful sets of test inputs, to create test cases able to reach most potential faults and even to guarantee the program state infection for predicates. However, they do not ensure the creation of test cases able to reveal faults. In particular they do not consider the propagation and revealability conditions of the RIPR model. As an extreme example, we could remove all the assertions in a test suite. The test suite will be able to cover the same number of statements as before while it will loose its ability to evaluate the result of the tests. See for instance, that removing the assertion invocation in <<ripr-test>> leaving the method invocation, will produce the same coverage for `findLast`, but it will not uncover the fault anymore. Statement coverage, is useful to discover the code that is not tested but it is not good when it comes to evaluate the capability of the test suite to reveal bugs, which is, in the end, the main purpose of testing.

_Mutation testing_ or _mutation analysis_ is a coverage criterion that directly assesses the capability of a test suite to discover bugs. It was first proposed in by DeMillo _et.al._ in the late 70's of the last century <<demillo1978hints>>.

The idea behind this criterion is simple: if a test suite should reveal bugs, then we can assess how effective it is by using artificial faults. That is, we insert  artificial faults in the program and then we execute the test suite for each fault. The test suite should fail under these conditions, otherwise it needs to be improved.

By inserting the artificial fault we _mutate_ the original program to create a faulty version. This version is called a _mutant_. If there is at least one test case in the test suite that fails when it is executed with the mutant, then the mutant is considered as detected and it is said to be _killed_. Otherwise it is a _live_ mutant. As a coverage criterion, mutation testing targets all live mutants as test requirements. This means that we design test cases to fail with those mutants that are not detected by the current set of test cases.

The _mutation score_ is the ratio of killed mutants to the total number of mutants used. It is the the coverage level for this criterion and it is often used to quantitatively evaluate the bug revealing capabilities of a test suite. We aim to design test suite with high mutation score.

In general, mutation testing works under following two assumptions: 

- Programmers create programs that are close to being correct (_The competent programmer hypothesis_) and 
- A test suite that detects all simple faults can detect most complex faults. That is, complex errors are coupled to simple errors. (_The coupling effect_)

So, to create a mutant, mutation testing inserts only small and well localized syntactic changes. These changes follow predefined fault or transformation models called _mutation operators_. A mutation operator could, for example, change `+` by `-` in an arithmetic expression. Other operators may change a comparison operator, for example `>` by `>=`, change a constant in the code, for example, change any constant by `0`, remove a method invocation and so on.

<<mutation-testing-algorithm>> shows in pseudo-code how mutation testing works. The procedure takes as input a program under test, the test suite designed to verify the program and a set of mutation testing operators. All mutation operators are used to create a set of mutants. Each mutant is then challenged against the test suite. If the test suite does not fail when executing a mutant, the faulty program version is kept as a _live mutant_. In the end, the procedure reports the list of live mutants and the mutation score.

//TODO: explain the mutation testing algorithm
[[mutantion-testing-algorithm, Listing {counter:listing}]]
.Listing {listing}. Pseudo-code of the mutation testing process.
[source, python]
----
input: program, test_suite, mutation_operators
ouput: live_mutants = {}, 
procedure:
    for mutant in generate_mutants(program, mutation_operators): #<1>
        fails = execute(test_suite, mutant) #<2>
        if not fails:
            live_mutants.append(mutant)
    mutation_score = size(live_mutants)/size(mutants)
    return  live_mutants, mutation_score #<3>
----
<1> Create mutants based on the mutation operators.
<2> Execute the entire test suite with the mutant. `fails` is `false` if no test case failed in the execution.
<3> The procedure returns the set of live mutants and the mutation score. 

To illustrate the functioning of mutation testing we go back to the code of <<stack-example>>. We will consider the `BoundedStack` class as the program under test. <<mutatns-test-suite-example>> shows the test suite we shall evaluate with the help of mutation testing. This test suite contains only two test cases: one verifying the `push` method and another verifying the `pop` method. 

[[mutants-test-suite-example, Listing {counter:listing}]]
.Listing {listing}. Test suite to evaluate using mutation testing.
[source, java]
----
public class BoundedStackTest {
    @Test
    public void testPush() {
        BoundedStack stack = new BoundedStack(1);
        stack.push(1);
        assertEquals(1, stack.size());
    }

    @Test
    public void testPop() {
        BoundedStack stack = new BoundedStack(1);
        stack.push(1);
        stack.pop();
        assertEquals(0, stack.size());
    }
}
----

We use the following four mutation operators:

- Negate the boolean expression of conditional statements.
- Replace `+` by `-` in arithmetic expressions.
- Replace `-` by `+` in arithmetic expressions.
- If a method returns an integer, we change the return value by `0`.

<<mutants-stack-example>> shows the same `BoundedStack` class as in <<stack-example>> and highlights are the locations where the artificial faults are inserted to create the mutants.

[[mutants-stack-example, Listing {counter:listing}]]
.Listing {listing}. Examples of mutants created for the `BoundedStack` shown in <<stack-example>>.
[source, java]
----
public class BoundedStack {
    private int[] elements;
    private int count;

    public BoundedStack(int capacity) {
        elements = new int[capacity];
        count = 0;
    }

    public void push(int item) {
        if(count == elements.length) { //<1>
            throw new IllegalStateException();
        }
        elements[count] = item;
        count = count + 1; //<2>
    }

    public int pop() {
        if(count == 0) { //<3>
            throw new NoSuchElementException();
        }
        count = count - 1; //<4>
        return elements[count]; //<5>
    }

    public int size() {
        return count; //<6>
    }

    public int capacity() {
        return elements.length; //<7>
    }
}
----
<1> Negate the conditional: `count != elements.length`.
<2> Replace `+` by `-`: `count = count - 1`.
<3> Negate the conditional: `count != 0`.
<4> Replace `+` by `-`: `count = count + 1`.
<5> Replace the result by `0`: `return 0`.
<6> Replace the result by `0`: `return 0`.
<7> Replace the result by `0`: `return 0`.

With the four mutation operators we obtain seven mutants in total. Two mutants, (<1> and <3>) are produced by negating conditionals inside `push` and `pop`. A mutant (<2>) is produced by changing the arithmetic operator in the increment inside `push` and another by changing the decrement inside `pop` (<4>). The other three are created by changing the result of `pop`, `size` and `capacity`. No fault was inserted in the constructor of the class.

Now, each fault is inserted in isolation from the rest and that each mutant is challenged by the execution of the test suite. The outcome for each mutant is as follows:

- Mutant *1* is killed. It makes `pop` throw an unexpected exception which also makes both test cases `testPush` and `testPop` fail.
- Mutant *2* is killed. After the fault is executed, `count` results in `-1` then the assertion in `testPush` fails as it expects the result of `size` to be 1 and not `-1`;
- Mutant *3* is killed in the same way as mutant <1>: `pop` throws an unexpected exception and `testPop` fails.
- Mutant *4* is killed. At the end of the invocation of `pop` in `testPop`, `count` is equal to `2` which is not the expected value per the assertion and the test case fails.
- Mutant *5* lives. It is not detected by any of the two test cases. In the code of the mutant `pop` returns `0`. However, the `testPop` does not verify the result of the method. Therefore the test case does not fail and the mutant lives.
- Mutant *6* is killed. The result of `size` changed to return `0`. This does not make `testPop` fail but it does make `testPush` fail as the expected value is `1`.
- Mutant *7* lives. The method is not invoked by any test case, therefore the mutated code is never reached and the mutant lives.

Out fo the seven mutants, two survived the analysis. Then, this small test suite has a score of stem:[\frac{2}{7}]. Inspecting both live mutants we can find hints on how to improve our test suite. 

The fault introduced to create mutant *7* is not executed by the test suite. In this sense we say that the mutant is not reached by the test suite. `capacity` is a simple getter, it might be a waste of time to create a test case only to verify it but it exposes part of the internal state of a `BoundedStack` instance. The fact that it is not executed means that we are not verifying this part of the state. So, instead of creating a test case designed to check the functionalities of `capacity` it might be better to check that the other methods do not affect the part of the state that `capacity` exposes. In the case of our example we might verify that the capacity of the stack never changes. See that unreached mutants produce the same information as code coverage.

On the other hand, mutant *5*, is reached by the test suite. The artificial fault actually infects the program state as the result of `pop` is changed in the execution from `1` to `0`. However the oracle in `testPop` does not check the result of `pop` and it becomes evident that we should either add a new assertion for this or even create a new test case designed to check the result of the method.

Mutation testing evaluates the test suite as a whole. So, it is not required to improve `testPop` to detect mutant <6> as it is detected by `testPush`. In the same way no test case can cover all scenarios, all test cases shall not kill all mutants.

Although it has gained some attention in the last few years and in spite of being several decades old, mutation testing has still a scarce adoption in industry when compared with code coverage. The main reason for that lay in the limitations of the criterion:

- The mutation testing process is complex. There is a true lack of tooling support with easy integration in the development process beyond academic circles. However, this has been changing with emergence of industrial level tools like PIT, for Java programs <<coles2016pit>>. 
- Mutation testing is very expensive in terms of execution time. This is one of the main arguments used against it. The test suite has to be executed for each mutant that is created in the process. The number of mutants, even for very small programs is potentially huge. Also, if the mutants are inserted in the source code then the program needs to be recompiled after the fault is inserted every execution, which increases even more the execution costs.
- It is hard to interpret the mutation score as a global metric for the quality of the test suite. Code coverage is simpler, it represents the percentage of code that is executed by the test suite. However, the mutation score is tied to the mutation operators we use. It does not mean that we can catch that portion of all possible faults.
- Live mutants need to be inspected by hand which may be tedious and difficult. The manual inspection has to main goals: understand why the mutant is not killed and obtain hints to improve the test cases and, to sort out mutants equivalent to the original code. Equivalent mutant are a major problem in mutation testing. These are artificial faults that turn out to be functionally equivalent to the original code. They pollute the value of the mutation score and waste the time of developers. In the general case, there is undecidable to determine if a mutant is equivalent to the original program.

Along the years, the community has developed many strategies to cope with the limitations of mutation testing. First, equivalent mutants my not be totally negative. According to Henry Coles, the creator of PIT, they can be often an indication that the original program is redundant and that it could be refactored. <<equivalent-mutant>> shows an example of this phenomenon explained by Coles in <<coles2018making>>.

The first method corresponds to the original method under test. The second is the mutant created by changing the comparison operator or the second conditional statement. The mutant is equivalent to the original code. The case where `i` is equal to `100` is already contained in the first condition. However, this situation reveals that the second condition is redundant and that it could be removed to make the code of the method simpler.

[[equivalent-mutant, Listing {counter:listing}]]
.Listing {listing}. Example of an equivalent mutant and how it signals the need for refactoring.
[source, java]
----
//Original code
public void someLogic(int i) { //<1>
  if (i <= 100) { 
    throw new IllegalArgumentException(); 
  } 
  if (i > 100) {
    doSomething(); 
  } 
}

//Mutant
public void someLogic(int i) { //<2>
  if (i <= 100) { 
    throw new IllegalArgumentException(); 
  } 
  if (i >= 100) {
    doSomething(); 
  } 
}

// Refactored code
public void someLogic(int i) { //<3>
  if (i <= 100) { 
    throw new IllegalArgumentException(); 
  } 
  doSomething();  
}
----
<1> Original method code.
<2> A mutant is created by changing the comparison operator from `>` to `>=`. The mutant is equivalent to the original code.
<3> Refactored method.

There are several strategies to reduce the cost of executing the mutation analysis. Here are some of them:

- We can use a statistically significative sample instead of all mutants created in the process.
- We may use only a handful of mutation operators. In fact, some research results indicate that using mutation operators that only remove statements produces much fewer mutants making the analysis more efficient and still highly effective <<deng2013empirical>>.
- The execution of the test suite  against each mutant can be parallelized, for example in concurrent processes or even in more specialized infrastructures like clusters.
- The test suite execution can stop after the first test case failure. In general there may be no need to run the rest of the tests once one of them has failed as the mutant is already detected.
- When executing the test suite with a mutant, we should consider only the test cases reaching the mutant and not the entire test suite. As a consequence, the test suite will not be executed if a mutant is not reached by any test case. This strategy requires computing beforehand the statement coverage, but it has been proven to a huge real time saver in practice with tools like PIT.
- To avoid compilation cycles, mutants could be inserted in-memory in the compiled code. Another strategy inserts  the code of all mutants at once with only one compilation cycle. Mutants are then activated with the help of external parameters <<untch1993mutation>>.

Mutation testing is a strong coverage criterion to evaluate and improve a test suite. It can produce the same information as code coverage but leads to a more meaningful hints for the creation and hardening of test cases. This is particularly true if live mutants are analyzed with the RIPR model. A mutant that does not infect the state of any test case execution signals the need of additional test inputs. A live mutant that propagates the state infection to the test code signals the need for better oracles as seen in the examples above. In this sense, it is the strongest coverage criteria explained in this chapter. However, this strength comes at a high cost as evidenced by the limitations discussed before.

===== Coverage criteria in practice


All coverage criteria discussed before are used in practice. This section presents some examples on how are they used in industry along with some of the available tools for their computation.

Logic coverage criteria are often used for embedded systems. In particular Modified Condition /Decision Coverage (MC/DC) is included as a requirement in the DO-178C standard: _Software Considerations in Airborne Systems and Equipment Certification_. It is the main guideline emitted by the corresponding certification authorities to approve commercial software-based aerospace systems. The same coverage metric is highly recommended by the ISO 26262 standard: _Road vehicles – Functional safety_: an international regulation for the functional safety of electrical and electronic systems in serial production road vehicles.

//TODO: Reference?

Structural coverage criteria applied to code artifacts, that is, code coverage, are arguably the most used criteria in practice. The coverage outcome is easy to interpret: code coverage signals the code locations not executed by the test suite, so we need new test cases to reach them. There are also many available tools able to compute coverage efficiently and these tools can be easily integrated in the development process either in IDEs or as a build step of CI builds.

For Java projects three of the most used tools to compute code coverage are JaCoco\footnote{\url{https://www.jacoco.org/jacoco/}}, Cobertura\footnote{\url{https://cobertura.github.io/cobertura/}} and OpenClover\footnote{\url{http://openclover.org/}}. These tools instrument the original code to insert instructions to store the coverage information. JaCoCo and Cobertura instrument the compiled bytecode, while OpenClover instruments the source code before compiling it using Aspect Oriented Programming. As an example, <<coverage-instrumentation>> shows how OpenClover instruments the `BoundedStack` class from <<stack-example>> to compute coverage. The tool creates a custom inner class `__CLR4_4_100kheirnt4` in charge of recording the coverage informations. Then it inserts `__CLR4_4_100kheurnt4.R.inc` invocations to record the code location once it is executed. See how these invocations identify each code location by a number and how similar actions are taken inside the conditions. The instrumentation does not harm the execution time of the code, so computing coverage is as efficient as executing the test suite.

[[coverage-instrumentation, Listing {counter:listing}]]
[source, java]
.Listing {listing}. Extract of the `BoundedStack` class from <<stack-example>> instrumented by OpenClover to compute coverage.
----
class BoundedStack {
    public static class __CLR4_4_100kheurnt4 { ... }
    ...
    public void push(int item) {
        try{
            __CLR4_4_100kheurnt4.R.inc(3);
            __CLR4_4_100kheurnt4.R.inc(4);
            if(((count == elements.length) && (__CLR4_4_100kheurnt4.R.iget(5)!=0|true))
                || (__CLR4_4_100kheurnt4.R.iget(6)==0&false)) 
                {
                    {
                        __CLR4_4_100kheurnt4.R.inc(7);
                        throw new IllegalStateException();
                    }
                }
                __CLR4_4_100kheurnt4.R.inc(8);
                elements[count] = item;
                __CLR4_4_100kheurnt4.R.inc(9);
                count = count + 1;
        }
        finally{
            __CLR4_4_100kheurnt4.R.flushNeeded();
        }
    }
        ...
}
----

Tools like JaCoCo, Cobertura and OpenClover are able to report line coverage, statement coverage, basic block coverage and branch coverage. See in <<coverage>> and example of a report generated by OpenClover for `BoundedStack`. On the left we have the line numbers. The other number indicated how many times the line has been executed by the test suite. The green code indicates areas that are covered while the red color indicates a coverage issue. Notice that line 15 is reported as not executed. Line 14 on the other hand is reported as executed twice but the tool also reports that only the `false` branch of the condition has been executed.

[[coverage]]
[#coverage.text-center]
.An example of a coverage report produced by OpenClover. 
image::openclover-coverage.png[Coverage report example produced by OpenClover]


Code coverage has been widely adopted by software companies. Google, for example, has its own guidelines when interpreting and using code coverage. They can be consulted for more details in <<arguelles2020code>> and <<ivankovic2019code>>. In these guidelines, Google developers express that there is no “ideal code coverage number” that can be applied to all software products. The coverage level depends on the relevance of the code, how often a code location changes and how long is the code expected to be used. In broad terms they consider a 60% coverage as _acceptable_, 75% as _commendable_ and 90% as _exemplary_. They argue that developers should aim at improving code coverage, but the effort should be directed to improve badly covered code, for example to go from 30% to 70% instead of going from 90% to 95% in a project. Raising an already very high code coverage may require a lot of effort with little gain. Coverage metrics are used inside the company together with static analysis to support code reviews.

These practices are not exclusive of big companies like Google. The XWiki Project\footnote{http://www.xwiki.org/} builds a Java platform to develop collaborative applications. The contributors of this project pay close attention to the quality of their test suite and have incorporated coverage monitoring into their development practices. The main XWiki codebase is composed by 3 Maven multi-module projects with more than 40 submodules each. These modules may contain more than 30K lines of application code and more than 9K of test code implementing several thousands of test cases. The entire development process is monitored in a https://ci.xwiki.org/[CI server running Jenkins].

The build pipeline in the CI includes actions to check the quality of the code and tests. In particular, they monitor the coverage achieved by the test suite. Each module in the codebase has a predefined threshold and the code coverage can not decrease below this value, otherwise the build will fail. In this way, if a developer adds some code she has to also provide new tests cases so the coverage ratio remains above or equal the predefined value. If a developer achieves a coverage above threshold, then she is given the possibility to raise the value for the module. In this way it is ensured that the code coverage never decreases. This is what they call the _Ratchet Effect_. This strategy has led to an effective use of the code coverage metric and a substantial increment on coverage levels <<massol2013tip>>. 

Mutation testing has not gained an adoption as broad as code coverage has. The mutation analysis provides a better criterion for the quality of the tests but it is a complex and expensive process, sometimes hard to interpret. In terms of tooling, there are a few options available. Among them, the most popular alternative is http://pitest.org[PIT or PITest]. PIT instruments mutants in the compiled bytecode and integrates with all major build systems like Ant, Gradle and Maven. Other tools for Java include https://www.st.cs.uni-saarland.de/mutation/[Javalanche] which also manipulates bytecode to inject faults and http://mutation-testing.org/[Major] that operates at the source code level. Major is integrated with the Java compiler and provides a mechanism to define new mutation operators.

PIT was develop for industry use unlike many other tools that have been developed mainly for research purposes. Apart for a good integration with most build systems, the tool has a pluggable architecture allowing its configuration and extension. The implements many of the traditional mutation operators like: changing instances of comparison operators, arithmetic operators, removing method invocations, perturbing method results and many others. PIT also implements many strategies to make mutation testing affordable. For example it creates mutants in memory from the compiled code and not the source code. It computes code coverage before running the analysis, so only mutants reached by test cases are inspected and only the test cases reaching each mutant are executed. The test cases are also prioritized, so faster test cases are executed first.

Mutation testing has gained some traction in the last few years. Companies like Google have started using it. Petrovic and Ivankovic <<petrovic2018state>> have described how the company uses mutation analysis in their codebase. They explain that the Google repository contains about two billion lines of code and on average, 40000 changes are incorporated every workday and 60\% of them are created by automated systems. In this environment it is not practical to compute a mutation score for the entire codebase and vit is very hard to provide an actionable feedback to developers

Most changes to the code pass through a code review process. So, the mutation testing feedback is incorporated into code reviews along with static analysis and code coverage. This eliminates the need for developers to run a separated program and act upon its output. To make the mutation analysis feasible the system shows at most one mutant by covered line. To further reduce the number of mutants, they classify each node of the Abstract Syntax Tree (AST) as important or non-important. To do this, they maintain a curated collection of simple AST nodes classified by experts. The system keeps updating this database with the feedback from the reviewing process. This selection may suppress relevant live mutants but there is a good tradeoff between correctness and usability as as the number of potential mutants is always much larger than what can be presented to reviewers.

The system analyses programs written in C++, Java, Python, Go, JavaScript, TypeScript and Common Lisp. It has been validated with more than 1M mutants in more than 70K diffs. 150K live mutants were presented and 11K received feedback. 75% of the findings with feedback were reported to be useful for test improvement. The company has also noticed differences related to the survival ratio of mutants when contrasted with the programming language and mutation operator.

==== Test doubles

Often, code units depend on other software components to achieve their functionalities. When testing a unit it may happen that some of its dependencies are not yet implemented and we only have an interface. It could also happen that these dependencies perform actions that can't be undone, like sending an email to a customer which makes them not suited for testing. In such scenarios we use test doubles. Doubles as in stunt doubles.

Ammann and Offutt <<amman2017introduction>> define test doubles as software components implementing only partial functionalities that are used during testing in place of actual production components. That is, we create classes that do not contain the actual functionality but provide a minimal working code that we can leverage in our tests.

Let's illustrate test doubles with a simplified example. Suppose we are building an e-commerce website. In our code (<<test-doubles-example-scenario>>) we have notions about customers, products and shopping carts. A product has a base price. A shopping cart belongs to a user and contains a collection of products. This class uses a service `PromoService` to obtain the active promotions for a user. A promotion applies to a product and informs the net discount for a product with `getDisscountFor`. A shopping cart computes the final price of the collection of products using the active promotions and applies all discounts.

[[test-doubles-example-scenario, Listing {counter:listing}]]
[source, java]
.Listing {listing}. Components of a simplified e-commerce website.
----
class User { 
    ... 
}

class Product { 
    ...

    public BigDecimal getBasePrice() {
        ...
    } 
}

class ShoppingCart {

    User user;
    Collection<Product> products;
    PromoService service;

    public BigDecimal getTotalBasePrice() {
        // Sum of all base prices
        ...
    }

    public BigDecimal getFinalPrice() {
        ...
    }

}

interface PromoService {

    Collection<Promotion> getActivePromotions(User user);

}

interface Promotion {

    boolean appliesTo(Product product);

    BigDecimal getDiscountFor(Product product);

}

----

Suppose we are given the task to test `getDiscountFor`. `ShoppingCart` depends on `PromoService`. The actual implementation of this interface could be still under development at the moment we will test `ShoppingCart`. It could also be the case, that the actual implementation requires access to a database which might make the test slower. To avoid dealing with these issues we can create test doubles for `PromoService`.

For example, one scenario we would like to test is when the user has no active promotions. Instead of messing with the actual production code, we can create a simple test double like in <<empty-service-stub-example>>. This double returns a predefined constant value. Such test doubles are usually called _stubs_. Apart from returning fixed values, stubs could also return values selected from a collection, or values that depend on the input or even random values. 


[[empty-service-stub-example, Listing {counter:listing}]]
[source, java]
.Listing {listing}. A simple test double for `PromoService` to test the scenario where the user has no active promotions and an extract of a test case using the test double.
----

class NoPromoServiceStub implements PromoService {
    public Collection<Promotion> getActivePromotions(User user) {
        return Collections.empty();
    }
}

...

@Test
public void testPriceNoActivePromotion() {
    ShoppingCart cart = ...
    cart.setPromoService(new NoPromoServiceStub());

    assertEquals(cart.getTotalBasePrice(), cart.getFinalPrice(), 
                "With no promotions the final price must be equal to the total base price");
}

----

Since we are testing the interaction between software components, sometimes it is useful to add verifications inside the code of a test double. This could help us verify, for example, that a method has been called a certain number of times. In our example, we could verify that `getActivePromotions` is invoked only once. Test doubles implementing verifications on the interaction of components are usually called _mocks_.


NOTE: Some authors state that mocks are a kind of specialized stubs. As Ammann and Offutt say, whether a mock is also a stub is not relevant for practical purposes. Other authors may include more classifications for test doubles such as: _dummies_, _spies_ and _fakes_, depending on the amount of functionality they implement.


Test doubles have several advantages in practice. We already discussed that test doubles are helpful when a component has not been implemented to create test cases matching the specification. Then, the implementation can leverage these early tests and the development of other functionalities is not blocked. As we discussed before, test doubles also avoid the execution during testing of actions that are permanent or hard to undone, like writing to a database or sending an email. Test doubles also provide a nice sandbox when dealing with external components that may not always be available or their execution takes a lot of time, for example, using a remote REST API. In such scenarios, using test doubles guarantee that our test cases will remain fast and will not fail if the service is not available.

We can choose to create test doubles manually, like we did before with the stub. In such a simple example manually creating the stub was enough. But, for more complex scenarios and to perform more complex verifications inside mocks we might better use available _mocking frameworks_.

===== Mocking frameworks: Mockito

Our tests should be simple to understand, as we need to map them to the specification and we need to maintain them. Creating test doubles by hand could make our test cases more complex. _Mocking frameworks_ are software tools that keep the creation of test doubles clean and easy to incorporate to our test code. One of such frameworks for Java is https://site.mockito.org/[_Mockito_]

Mockito has an easy to use and understand fluent API. With it we can create and verify stubs and mocks almost declaratively. <<empty-mockito-stub-example>> shows an example of who to rewrite <<empty-service-stub-example>> with Mockito.

[[empty-mockito-stub-example, Listing {counter:listing}]]
[source, java]
.Listing {listing}. Rewriting <<empty-service-stub-example>> with the help of Mockito.
----
@Test
public void testPriceNoActivePromotion() {
    
    PromoService service = mock(PromoService.class); //<1>

    when(service.getActivePromotionsFor(any())) //<2>
        .thenReturn(Collections.emptyList());
    
    ShoppingCart cart = ...
    cart.setPromoService(service);

    assertEquals(cart.getTotalBasePrice(), cart.getFinalPrice(), 
                "With no promotions the final price must be equal to the total base price");
}

----
<1> Instructs Mockito to create a test double for `PromoService`.
<2> Specifies the behavior on `getActivePromotionsFor`. The double is instructed to return an empty list for any argument.

<<empty-mockito-mock-example>> shows an extract of a test case verifying that `getFinalPrice` invokes `getActivePromotionsFor` only once. If during the execution of the test case, the method is not invoked or it is invoked two or more times, the test case will fail.

[[empty-mockito-mock-example, Listing {counter:listing}]]
[source, java]
.Listing {listing}.
----
@Test
public void testPromotionsConsutledOnce() {
    PromoService service = mock(PromoService.class);
    ShoppingCart cart = ...
    cart.setPromoService(service)
    BigDecimal finalPrice = cart.getFinalPrice();

    ...
    
    verify(service, times(1)).getActivePromotionsFor(any()); //<1>
}
----
<1> Assertion to verify the number of times the method was invoked.

In both examples Mockito prevented the manual creation of classes and provided an easy to understand mechanism to incorporate the test doubles. The test code remains simple and self-contained. Also, the same general mechanism can be used for all our classes. The framework also allows mocking concrete classes beyond only interfaces. It also provides ways to verify that a method was invoked with specific arguments, and even to still invoke concrete methods while observing their behavior.

===== Best practices and misuses

Test doubles are powerful allies for testing, but they can be often misused. There are even testing smells that concern only mocks and stubs. 

One of the test smells coined for mocks is the _Mock Happy_ smell. It refers to test cases that require many mocks to run. These test cases are often fragile and hard to understand <<garousi2018smells>> and maintain.

In general terms, the overuse of mocks can damage the quality of our test code. For example, we should avoid mocking value types or any other immutable type or any class that is simple enough to control and observe. Mocking those classes may result in a test code that brings little value and it may even be harder to understand than using the original class directly.

We should avoid mocking third party classes as much as possible. When we mock a class we make assumptions on their behavior. Those assumptions may be wrong if we don't fully understand how external classes behave in all scenarios. We should create mocks mostly for our own code. Expressing an incorrect behavior of external dependencies with mocks leads to test cases that do not fail, yet, the code will break in production <<freeman2007everything>>. 

=== The role of testability

High quality test cases are concise, clear, repeatable, independent, efficient and maintainable <<meszaros2003test>>. A good test suite facilitates the development of our product as it catches bugs earlier. However, code quality has also an important effect over the quality of the test cases. Low quality code is hard to test. If the classes in the code are tightly coupled, or the software has hidden external dependencies, or if it forcibly depends on external services, then it is hard for test cases to instantiate the components and set them in the specific states required for testing <<day2014available>>.

Therefore, we should design software with tests in mind, that is, we should design software to be testable. _Testability_ is the property of a system that characterizes how easy it is to test. It can be defined in terms of two other sub-properties: _controllability_: how easy it is so set a software component in the desired state and, _observability_: how easy it is to observe and verify the state of the component. In other words, a software is testable if its state is easy to control an observe. High quality code leads to a highly testable software.

For example, loosely coupled classes with high cohesion, that observe the _Single Responsibility Principle_ are easier to instantiate and also lead to easier and more understandable test cases. Complex code will necessary produce complex and hard to understand test cases. Coupled classes will also make the test code unnecessarily large and harder to read since we will need to create many objects at the same time to test one of them.

These symptoms of bad code often manifest themselves during testing. In most occasions it is better to go back and redesign our software components than creating complex test cases. For example repeating the same mocking setup across many test cases might be a signal of tight coupling that should be better solved in the application code than making the test code repetitive <<miller2008patterns>>.

Design patterns are general solutions to recurring problems in programming <<source2020design>>. These solutions are proven to be effective and, if well used, they help making the code easier to understand. The good application of design patterns also make the code more testable <<day2014available>>, <<moore2019designing>>. Two patterns often used as examples of testing facilitators and in particular to make mocking easier are _Dependency Injection_ and _Repository_.

Using _Dependency Injection_, classes receive their dependencies instead of explicitly create them. Classes then depend on abstractions, interfaces, that can be easily exchanged. Also, since classes are not in charge of creating their dependencies their code is simpler. Our example with `PromoService` is loosely based in this design pattern. This pattern makes it extremely easy to exchange the dependency, which is helpful for mocking and stubbing as we saw in the example. It also makes the component more controllable as we can create dependencies more easily than production code.

With the _Repository_ pattern we create a class that manages all access to the data. This class encapsulates and hides the actual data access operations, separating even more our components from the data access implementation and the concrete storage method used in production. Using this design pattern facilitates mocking the data access mechanisms which in turn facilitates the verification of how our code interacts with the data components. We also avoid making actual database modifications that we might need to undo to keep our tests repeatable.

=== Test Driven Development

_Test Driven Development_ (TDD) is an approach to software development that encourages writting tests first, before writing any application code. The concepts behind TDD are derived from the _Extreme Programming_ movement in the late 90's. The popularization of this methodology is credited to Kent Beck, who also created SUnit, a unit testing framework for Smalltalk and inceptor of the entire xUnit framework familly. Beck also participated in the development of JUnit together with Erich Gamma.

In his seminal book, _Test-driven development: by example_ <<beck2003test>> Beck states that TDD follows two simple rules:

* Don’t write a line of new code unless you first have a failing automated test.
* Eliminate duplication.

These two rules induce the organization of programming tasks in a cycle, knonw as the _Red/Green/Refactor_ cycle:

Red:: Write a simple and short test that does not work. Since the application code isn't already entirely written, the test might not even compile. The name of this phase makes allution to the fact that failing tests are usually shown in red in many programming environments.

Green:: Take any quick coding action required to make the failing test work. In this phase we will introduce low quality code with repetitions and non-optimal solutions. Our only goal is to make the test pass, hence the _Green_ name.

Refactor:: Eliminate any redundancy or duplication and improve the code added to make the test pass by refactoring. This refactoring benefits for the previously created tests. If we make a mistake during this phase, the tests in place will catch it early enough.

If we use TDD, whenever we add a new feature to our product we must start this cycle. At the end of each phase of the cycle we must execute all tests that have been created and not only the one we just addded. In the _Red_ phase only the new test should fail for the expected reasons. If the test does not fail, then it is wrong or does not bring any value to the test suite. No other test should fail by the addition of this new test. After the _Green_ phase all tests must pass. The new code should not brake any existing test and it must be created to make the new test pass. During the _Refactoring_ phase the test suite becomes a safety net in which we can rely to improve the quality of our code without introducing changes that break the expected behavior.

TDD requires an efficient test suite that can be executed regularly to obtain early feedback from tests. It also requires developers to write their own test cases. These premises are actually at the core of modern software development.

The methodology has true practical advantages <<moore2019designing>>. On the one hand, it forces the creation of tests that might otherwise never been written. On the other hand, the application code is designed from the users' perspective and it enforces developers to write testable application code. The effective use of TDD shall produce in the long term code with fewer bugs and shall reduce the need for debugging as tests should be informative enough to diagnose any fault.

Like any other methodology TDD has gained adepts and detractors which regularly engage in never-ending discussions. Both sides use strong arguments to make their cases. However, we must keep in mind that, again, as any practical approach, TDD fits better the practices of ones and not the ways of others. We should not use it blindly and we should think whether its application may bring more damage than good.

Applying TDD does not mean that we don't need a careful design of our product. An initial and comprehensive design phase is crucial for the quality of our software <<li2018why>>. A good design actually facilitates the application of TDD, as it helps keeping the tests focused. But it is not true in the opposite sense. Without a clear desing, we will be making the design on the fly and TDD may lead to bad design choices.

Real life is not the same as the small and carefully crafted examples, often used to introduce TDD. In those examples, the requirements are clear and will not change. However, in a real development projects the requirements may change in time and are often vague. We should take that into account otherwise we will find ourselves adapting our code and our prematurely created tests many times increasing the cost of maintainnace and development <<fox2019test>>.

TDD favors a strong focus on unit testing. Over-specifying our code with unit tests may lead to unuseful test cases that may well verify assertions that under no circumstances will be false. It may also lead to an important ammount of tests for very simple code. It may also induce to the overuse of mocks, whose perills we already discussed in the previous section. In practice, there are scenarios when a couple of higher level tests can be more useful than tens of unit tests. This tradeoff should be carefully taken into account <<schranz2014case>>.

Projects with well defined functional requirements unlikely to change much seem to be a good match for TDD. Take as example the creation of a compiler. The specification of the source and target language do not change much in time. The structure of the tests is also well define in the for of input and expected output. In such a project we will be creating tests cases directly matching the specification and main goal of the product.

Maybe, as Eric Gunnerson puts it, the most relevant phase of TDD is not creating the tests but the regular code refactoring. Refactoring should lead to a better designed product with high quality code, with low coupling and better testability <<gunnerson2017notdd>>.

But, should we use TDD or not? There is no simple answer. We must use the approach that better fits our practices. In the words of Martin Fowler:

[quote, Martin Fowler, Is TDD Dead? <<fowler2014is>>]
Everything still boils down to the point that if you're involved in software development you have to be thoughtful about it, you have to build up what practices work for you and your team, you can't take any technique blindly. You need to try it, use it, overuse it, and find what works for you and your team. We're not in a codified science, so we have to work with our own experience.

